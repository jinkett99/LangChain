{"context_similarity": [{"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9176038259688565", "score": 0.9176038259688565, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9176038259688565", "score": 0.9176038259688565, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9999999999999998", "score": 0.9999999999999998, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.8706482652857808", "score": 0.8706482652857808, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9810077187264207", "score": 0.9810077187264207, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9999999999999998", "score": 0.9999999999999998, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9999999999999998", "score": 0.9999999999999998, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9999999999999998", "score": 0.9999999999999998, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9209988037556023", "score": 0.9209988037556023, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9209988037556023", "score": 0.9209988037556023, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.8898591361042936", "score": 0.8898591361042936, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9204253166994112", "score": 0.9204253166994112, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9186957965301812", "score": 0.9186957965301812, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9138306548435005", "score": 0.9138306548435005, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9886802917151041", "score": 0.9886802917151041, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.8863668164340476", "score": 0.8863668164340476, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9302308317679263", "score": 0.9302308317679263, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9393354671457073", "score": 0.9393354671457073, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9131993401697003", "score": 0.9131993401697003, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9131993401697003", "score": 0.9131993401697003, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9223659744298962", "score": 0.9223659744298962, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.8842073407403518", "score": 0.8842073407403518, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.8608340337976272", "score": 0.8608340337976272, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9034119170319072", "score": 0.9034119170319072, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9216380072313756", "score": 0.9216380072313756, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9262134855296931", "score": 0.9262134855296931, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.8262905555508461", "score": 0.8262905555508461, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.8954540572597434", "score": 0.8954540572597434, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9999999999999999", "score": 0.9999999999999999, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9335191596824816", "score": 0.9335191596824816, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": false, "feedback": "Similarity score: 0.7937176454240278", "score": 0.7937176454240278, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9503470475634459", "score": 0.9503470475634459, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 1.0000000000000002", "score": 1.0000000000000002, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.8273387838179035", "score": 0.8273387838179035, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9999999999999999", "score": 0.9999999999999999, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": null, "contexts": null, "response": null, "passing": true, "feedback": "Similarity score: 0.9999999999999999", "score": 0.9999999999999999, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}], "correctness": [{"query": "**Question:** Who are the members of Group 9 in the context of the eyecatcher project report?", "contexts": null, "response": "The members of Group 9 in the context of the eyecatcher project report are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.", "passing": true, "feedback": "The generated answer is fully correct and relevant, providing the same information as the reference answer in a clear and concise manner.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The members of Group 9 are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.", "contexts": null, "response": "The members of Group 9 are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.", "passing": true, "feedback": "The generated answer is identical to the reference answer, providing the same information accurately and concisely.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the key components of the machine learning system architecture as outlined in the project report?", "contexts": null, "response": "The key components of the machine learning system architecture include:\n\n1. **Amazon S3 Training Bucket**: Stores training images converted into PyTorch Tensors for model training.\n2. **Amazon S3 Interim Bucket**: Stores reported or appealed images for moderators to evaluate.\n3. **Amazon SageMaker**: Used for model training, building, and compilation, as well as for model deployment.\n4. **AWS CodeCommit**: Stores source code and changes history.\n5. **AWS CodeBuild**: Compiles source code and builds the model.\n6. **AWS CodePipeline**: Automates the pipeline for continuous integration and continuous deployment (CI/CD).\n7. **Amazon CloudWatch**: Monitors model performance and logs, sending alarms as needed.\n8. **AWS Lambda**: Provides serverless computing for inference, updating data labels, and triggering model training pipelines.\n9. **API Gateway**: Facilitates interactions between public requests and AWS services. \n\nThese components work together to create a robust architecture for the machine learning system.", "passing": false, "feedback": "The generated answer lists several components relevant to a machine learning system architecture, but it diverges significantly from the key components outlined in the reference answer. It focuses on specific AWS services rather than the broader categories of model building, deployment, and monitoring as mentioned in the reference. While it is relevant to machine learning, it does not accurately reflect the structure or components specified in the user query.", "score": 2.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The key components of the machine learning system architecture outlined in the project report include model building, model deployment, and monitoring & retraining steps. Specifically, model building encompasses the workflow, data preprocessing, model training, and model quantization. Model deployment includes the deployment workflow, auto scaling policy, and deployment strategy. Finally, the monitoring & retraining step involves user feedback to handle concept and model drift, as well as the implementation of user feedback through a Discord server bot.", "contexts": null, "response": "The machine learning system architecture consists of several key components. Model building includes the workflow, data preprocessing, model training, and model quantization. For model deployment, it encompasses the deployment workflow, auto scaling policy, and deployment strategy. Lastly, the monitoring and retraining step focuses on user feedback to manage concept and model drift, along with the implementation of user feedback via a Discord server bot.", "passing": true, "feedback": "The generated answer is relevant and fully correct, matching the structure and content of the reference answer while maintaining clarity and conciseness.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the primary focus of the project described in the report, and how does it aim to address the challenges of content moderation on social media platforms?", "contexts": null, "response": "The primary focus of the project is to develop a deployable machine learning pipeline specifically for explicit image classification, with an emphasis on detecting explicit nudity. It aims to address the challenges of content moderation on social media platforms by providing a cost-effective tool that can accurately detect and filter out explicit images. This solution is designed to enhance user trust and safety while maintaining brand reputation. By leveraging advanced machine learning algorithms and cloud-native services, the project seeks to empower both users and community managers, enabling them to engage in content moderation effectively and efficiently. The system incorporates user feedback mechanisms to continuously improve its accuracy and adapt to changing content trends, thereby fostering a safer online environment.", "passing": true, "feedback": "The generated answer is highly relevant and accurately captures the primary focus of the project as well as its aims in addressing content moderation challenges. It mirrors the key points of the reference answer while maintaining clarity and conciseness.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The primary focus of the project is to develop a deployable machine learning pipeline for explicit image classification, specifically targeting explicit nudity detection. It aims to address the challenges of content moderation on social media platforms by providing a cost-effective tool that can accurately detect and filter out explicit images, thereby enhancing user trust and safety while maintaining brand reputation.", "contexts": null, "response": "The project is centered on creating a machine learning pipeline designed for explicit image classification, with a specific emphasis on detecting explicit nudity. It seeks to tackle the difficulties associated with content moderation on social media platforms by offering an affordable solution that can effectively identify and filter explicit images. This initiative aims to bolster user trust and safety while also preserving the reputation of brands.", "passing": true, "feedback": "The generated answer is highly relevant and fully correct, matching the content and intent of the reference answer while also maintaining clarity and conciseness.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What method did the team use for data labeling in their explicit image classification project, and what were the advantages of this method?", "contexts": null, "response": "The team used Amazon Rekognition's \"DetectModerationLabels\" method for data labeling in their explicit image classification project. The advantages of this method included improved data quality for training, validation, and testing while minimizing labeling costs. Additionally, since Rekognition utilizes an existing trained model for classification, it proved to be a more cost-effective solution compared to using human labelers through Amazon Ground Truth. This approach also allowed for the generation of multiple sub-classes/labels, enhancing the granularity of the dataset.", "passing": true, "feedback": "The generated answer is relevant and correctly identifies the method used for data labeling, as well as some advantages. However, it lacks some details present in the reference answer, such as the mention of resilience against content drifts and the specific sub-labels, which would have provided a more comprehensive understanding.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The team used Amazon Rekognition's \"DetectModerationLabels\" method for data labeling. The advantages of this method included improved quality of the dataset for training, validation, and testing while minimizing labeling costs. Additionally, it utilized an existing trained model for classification, making it more cost-effective compared to using human labelers through Amazon SageMaker Ground Truth.", "contexts": null, "response": "The team opted for Amazon Rekognition's \"DetectModerationLabels\" method for data labeling due to its ability to enhance the quality of the dataset used for training, validation, and testing while also reducing labeling costs. This method leverages an existing trained model for classification, which provides a more economical solution compared to employing human labelers through Amazon SageMaker Ground Truth.", "passing": true, "feedback": "The generated answer is fully relevant and correct, providing the same information as the reference answer while being slightly more concise.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the main components of the machine learning system architecture described in the document, and what is the purpose of each component?", "contexts": null, "response": "The machine learning system architecture consists of several key components, each serving a specific purpose:\n\n1. **Amazon S3 Training Bucket**: This component stores training images that will be converted into PyTorch Tensors for model training.\n\n2. **Amazon S3 Interim Bucket**: It stores reported or appealed images for moderators to evaluate and take appropriate action.\n\n3. **Amazon SageMaker \u2013 Training**: This is used for conducting model training, building, and compilation.\n\n4. **Amazon SageMaker \u2013 Model Registry**: It contains a catalog of models to track and manage different versions of the models.\n\n5. **Amazon SageMaker - Endpoint**: This component is responsible for deploying the model for real-time inference.\n\n6. **AWS CodeCommit**: It stores the source code and maintains a history of changes made to it.\n\n7. **AWS CodeBuild**: This service compiles the source code and builds the model.\n\n8. **AWS CodePipeline**: It automates the pipeline for continuous integration and continuous deployment (CI/CD).\n\n9. **Amazon CloudWatch**: This component monitors model performance through logs and sends alarms based on predefined metrics.\n\n10. **AWS Lambda**: A serverless computing service that performs inference, updates data labels, and triggers the model training pipeline.\n\n11. **API Gateway**: This managed service facilitates interactions between public requests and AWS services.\n\nThese components work together to create a robust machine learning pipeline that supports model training, deployment, monitoring, and management.", "passing": true, "feedback": "The generated answer is highly relevant and fully correct, matching the reference answer in both content and clarity. It accurately describes each component of the machine learning system architecture and their respective purposes without any mistakes.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The main components of the machine learning system architecture include:", "contexts": null, "response": "The main components of the machine learning system architecture include:\n\n1. **Amazon S3 Training Bucket** - Stores training images for model training.\n2. **Amazon S3 Interim Bucket** - Stores reported or appealed images for moderator evaluation.\n3. **Amazon SageMaker** - Used for model training, building, and compilation.\n4. **Amazon SageMaker Model Registry** - Contains a catalogue of models for tracking and management.\n5. **Amazon SageMaker Endpoint** - Deploys the model for real-time inference.\n6. **AWS CodeCommit** - Stores source code and change history.\n7. **AWS CodeBuild** - Compiles source code and builds the model.\n8. **AWS CodePipeline** - Automates the continuous integration and continuous deployment (CI/CD) pipeline.\n9. **Amazon CloudWatch** - Monitors model performance and sends alarms based on logs.\n10. **AWS Lambda** - Provides serverless computing for inference, data label updates, and triggering model training.\n11. **API Gateway** - Facilitates interactions between public requests and AWS services.", "passing": true, "feedback": "The generated answer is fully relevant and correct, matching the reference answer in both content and structure while maintaining clarity and conciseness.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon S3 Training Bucket:** Stores training images that will be converted into PyTorch Tensor for model training.", "contexts": null, "response": "The Amazon S3 Training Bucket is utilized to store training images, which are subsequently converted into PyTorch Tensors for the purpose of model training.", "passing": true, "feedback": "The generated answer is relevant and accurately describes the function of the Amazon S3 Training Bucket, including the conversion of images into PyTorch Tensors for model training. However, it lacks some detail about the preprocessing and the role of the bucket in the training pipeline, which is present in the reference answer.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon S3 Interim Bucket:** Stores reported/appealed images for moderators to evaluate and take appropriate action.", "contexts": null, "response": "The Amazon S3 Interim Bucket is designated for storing images that have been reported or appealed. This allows moderators to evaluate these images and take the necessary actions based on their assessments.", "passing": true, "feedback": "The generated answer is fully relevant and correct, matching the reference answer in both content and clarity.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon SageMaker \u2013 Training:** Conducts model training, building, and compilation.", "contexts": null, "response": "Amazon SageMaker \u2013 Training is responsible for conducting the model training process, which includes building and compiling the model.", "passing": true, "feedback": "The generated answer is relevant and correctly describes the role of Amazon SageMaker \u2013 Training, but it lacks detail about the various stages of training and the importance of model evaluation and registration mentioned in the reference answer.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon SageMaker \u2013 Model Registry:** Contains a catalogue of models to track and manage.", "contexts": null, "response": "The Model Registry serves as a centralized repository for managing and tracking various models. It allows for organized oversight of model versions, facilitating easier updates and maintenance throughout the model lifecycle.", "passing": true, "feedback": "The generated answer is relevant and correctly describes the purpose of the Amazon SageMaker Model Registry, focusing on its role as a centralized repository for managing models. However, it lacks some details present in the reference answer, such as the aspect of performance standards and the process of registering models after training and evaluation.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon SageMaker - Endpoint:** Deploys the model for real-time inference.", "contexts": null, "response": "The Amazon SageMaker Endpoint is responsible for deploying the model, enabling it to perform real-time inference. This allows the model to generate predictions based on incoming data as it is received, facilitating immediate responses in applications that require timely decision-making.", "passing": true, "feedback": "The generated answer is relevant and fully correct, accurately describing the function of Amazon SageMaker Endpoint in deploying models for real-time inference and emphasizing its role in generating predictions based on incoming data.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**AWS CodeCommit:** Stores source code and changes history.", "contexts": null, "response": "AWS CodeCommit is a service that is used to store source code and maintain a history of changes made to that code.", "passing": false, "feedback": "The generated answer is relevant to the user query as it accurately describes AWS CodeCommit's purpose in storing source code and maintaining change history. However, it lacks detail and does not mention its managed nature, scalability, or collaborative features, which are important aspects of the service.", "score": 3.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**AWS CodeBuild:** Compiles source code and builds the model.", "contexts": null, "response": "AWS CodeBuild is responsible for compiling the source code and building the model within the machine learning system architecture.", "passing": false, "feedback": "The generated answer is relevant to the user query and correctly identifies the role of AWS CodeBuild in compiling source code and building the model. However, it lacks additional context and detail about the service's capabilities and its role in continuous integration, which are present in the reference answer.", "score": 3.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**AWS CodePipeline:** Automates the pipeline for Continuous Integration/Continuous Deployment (CI/CD).", "contexts": null, "response": "AWS CodePipeline is a service that automates the pipeline for Continuous Integration and Continuous Deployment (CI/CD). It streamlines the process of building, testing, and deploying applications by integrating various stages of the development workflow, ensuring that code changes are automatically processed and delivered efficiently.", "passing": true, "feedback": "The generated answer is relevant and accurately describes AWS CodePipeline's function in automating CI/CD processes. However, it lacks the specific context of model training and continuous updates mentioned in the reference answer, which would enhance its completeness.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the key differences between the ResNet50 and Vision Transformer (ViT-16) models in terms of their architecture and performance based on the context provided?", "contexts": null, "response": "The ResNet50 model is a deep convolutional neural network that utilizes residual networks and introduces skip connections to mitigate vanishing gradient issues, allowing for the training of deeper networks. Although it achieved state-of-the-art results at its inception, it has since been surpassed by other architectures. Its performance accuracy is around 20%.\n\nIn contrast, the Vision Transformer (ViT-16) employs a transformer architecture for image classification, processing images by dividing them into patches and using self-attention layers to facilitate long-range interactions between image elements. This model does not rely on convolutional layers, enabling it to capture global dependencies effectively. The ViT-16 model demonstrates a significantly higher performance accuracy of approximately 60%.\n\nOverall, the key differences lie in their architectural approaches\u2014ResNet50 focuses on convolutional layers with residual connections, while ViT-16 leverages transformer mechanisms\u2014and their performance, with ViT-16 outperforming ResNet50 in accuracy.", "passing": true, "feedback": "The generated answer accurately captures the key differences in architecture and performance between ResNet50 and Vision Transformer (ViT-16), aligning closely with the reference answer. It provides a clear and concise comparison, detailing the architectural features and performance metrics of both models without any mistakes.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** ResNet50 is a deep convolutional neural network that utilizes residual networks and skip connections to address vanishing gradient problems, making it easier to train deeper networks. In contrast, Vision Transformer (ViT-16) employs a transformer architecture that processes images by dividing them into patches and using self-attention layers, allowing for long-range interactions between image elements and eliminating the need for convolutional layers. In terms of performance, ViT-16 significantly outperforms ResNet50, achieving an accuracy of approximately 60% compared to ResNet50's ~20%.", "contexts": null, "response": "ResNet50 is a deep convolutional neural network that employs residual networks and skip connections to tackle vanishing gradient issues, facilitating the training of deeper networks. On the other hand, Vision Transformer (ViT-16) uses a transformer architecture to process images by segmenting them into patches and applying self-attention layers, which enables long-range interactions among image elements and removes the necessity for convolutional layers. In terms of model performance, ViT-16 demonstrates a significant advantage, achieving around 60% accuracy, while ResNet50 reaches approximately 20%.", "passing": true, "feedback": "The generated answer is fully relevant and correct, matching the reference answer in content and clarity while maintaining conciseness.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the purpose of using a Canary deployment strategy in the context of the image classification model described in the report?", "contexts": null, "response": "The purpose of using a Canary deployment strategy for the image classification model is to enable an incremental rollout of new model versions to a small, controlled subset of users or environments before a full deployment. This approach allows for testing the model's performance, assessing potential risks, and identifying issues early without impacting the entire user base. It ensures a seamless experience for end-users by monitoring the new model's performance, measuring prediction accuracy, and detecting any unexpected behaviors or anomalies. Additionally, it facilitates a direct comparison between the new and existing model versions under real-world conditions, providing valuable feedback for decision-making regarding full deployment. The strategy also minimizes risks associated with deploying new models, allowing for quick rollbacks if problems arise, thereby protecting the overall system and user experience.", "passing": true, "feedback": "The generated answer is highly relevant and fully correct, accurately capturing the purpose and benefits of using a Canary deployment strategy in the context of the image classification model. It covers all key points mentioned in the reference answer, including performance monitoring, risk reduction, and the importance of incremental rollout, while maintaining clarity and coherence.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The purpose of using a Canary deployment strategy is to release new versions of the model to a small, controlled subset of users or environments before a full rollout. This approach allows the team to test the model's performance, assess potential risks, and detect issues early without affecting the entire user base. It enables monitoring of metrics such as prediction accuracy, model latency, throughput, and resource usage under real-world conditions, thereby reducing the risk associated with deploying new models and providing a safety net for high reliability and performance consistency.", "contexts": null, "response": "The Canary deployment strategy is designed to incrementally release new model versions to a limited group of users or environments prior to a complete rollout. This method facilitates performance testing, risk assessment, and early issue detection without impacting the entire user base. It allows for the monitoring of critical metrics such as prediction accuracy, model latency, throughput, and resource usage in real-world scenarios. By adopting this strategy, the team can minimize deployment risks and ensure a reliable and consistent performance of the model.", "passing": true, "feedback": "The generated answer is relevant and captures the essence of the Canary deployment strategy, closely mirroring the reference answer. However, it lacks some details present in the reference, such as the mention of comparing the new model with the existing one and the ability to quickly rollback if problems arise, which slightly reduces its completeness.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** How does the implementation of user feedback through the Discord server bot contribute to the model validation process in the image classification project?", "contexts": null, "response": "The implementation of user feedback through the Discord server bot plays a crucial role in the model validation process by facilitating community engagement and providing insights into the model's performance. When users upload images, the bot processes these images and sends them to various AWS services for classification. If an image is misclassified, users can appeal the decision, prompting moderators to review and confirm the labels. This feedback loop allows for the identification of wrongly classified images, which are then moved into the training bucket to create a ground truth dataset.\n\nAdditionally, the bot's auto-moderation actions for NSFW content help maintain community standards while also collecting data on user interactions and model performance. By regularly moving unreported images to the training bucket and triggering model retraining when misclassifications exceed a certain threshold, the system adapts to changes in user preferences and improves its accuracy over time. This iterative process of validation and retraining ensures that the model remains effective and relevant in a dynamic environment.", "passing": true, "feedback": "The generated answer is relevant and covers many aspects of how user feedback through the Discord bot contributes to model validation, similar to the reference answer. However, it lacks some specific details present in the reference, such as the automated monitoring process and the explicit mention of a threshold for retraining, which slightly reduces its completeness.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The implementation of user feedback through the Discord server bot contributes to the model validation process by allowing users to interact with the model's predictions. When users upload images, the bot sends the images for classification and stores the results along with metadata. If images are wrongly classified, users can appeal or report them, prompting moderators to verify the labels. This feedback loop helps identify and mitigate drift in the model's performance. Images confirmed to be misclassified are moved to the training bucket to form the ground truth, and when the number of wrongly classified images exceeds a predefined threshold, it triggers the model training pipeline, thereby continuously improving the model's accuracy.", "contexts": null, "response": "The user feedback mechanism implemented via the Discord server bot plays a crucial role in enhancing the model validation process. Users can upload images for classification, and the bot facilitates the submission of these images for analysis while also recording relevant metadata. In cases where images are misclassified, users have the option to appeal or report these instances, which leads to moderators reviewing and confirming the accuracy of the labels. This iterative feedback loop is essential for detecting and addressing any drift in the model's performance. Misclassified images that are verified are transferred to the training bucket to establish a reliable ground truth. Additionally, if the number of misclassifications surpasses a set threshold, it activates the model training pipeline, ensuring ongoing improvements in the model's accuracy.", "passing": true, "feedback": "The generated answer is highly relevant and fully correct, closely mirroring the reference answer in both content and structure while maintaining clarity and conciseness.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the significance of monitoring CPU utilization in the context of the image classification model, and what threshold was set for the alarm?", "contexts": null, "response": "Monitoring CPU utilization is crucial for understanding the computational demands of the image classification model. It provides insights that can lead to optimizations, such as reducing the model's computational load or upgrading to a larger instance type if necessary. This monitoring serves as an early warning system for potential disruptions due to insufficient computing power. The alarm threshold for CPU usage was set to more than an average of 70% over a 5-minute interval.", "passing": true, "feedback": "The generated answer is highly relevant and fully correct, matching the reference answer in both content and clarity while maintaining conciseness.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Monitoring CPU utilization is significant because it provides valuable insights into the computational demands of the model, allowing for timely optimizations or upgrades to ensure sufficient computing power. The alarm threshold was set to more than an average of 70% CPU usage over every 5-minute interval based on baseline percentages.", "contexts": null, "response": "Monitoring CPU utilization is crucial as it offers insights into the model's computational demands, enabling timely optimizations or upgrades to maintain adequate computing power. An alarm threshold has been established at over 70% average CPU usage for every 5-minute interval, based on baseline percentages.", "passing": true, "feedback": "The generated answer is relevant and correctly conveys the main points from the user query and reference answer. However, it lacks the additional detail about identifying potential disruptions and ensuring high-quality service, which is present in the reference answer.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What measures are proposed to monitor and manage Model Prediction Latency in the deployment infrastructure of the project?", "contexts": null, "response": "To monitor and manage Model Prediction Latency, the project plans to track the ModelLatency metric under the AWS/SageMaker namespace in CloudWatch. An acceptable threshold for latency will be defined based on the application's requirements, and CloudWatch alarms will be set up to notify the team if this limit is exceeded. This proactive approach aims to maintain the responsiveness of the service and ensure a seamless user experience. Additionally, during the staging phase with general users, latency data will be collected under real-world conditions to understand typical latency patterns, which will help in setting a realistic baseline for ModelLatency. The team will analyze trends in latency, considering both average latency and peak times, to ensure timely responses and effective management of user interactions.", "passing": true, "feedback": "The generated answer is relevant and covers most of the key measures proposed in the reference answer, including monitoring with CloudWatch, setting thresholds, and collecting data under real-world conditions. However, it lacks some details such as the establishment of a baseline and the specific mention of alarms, which slightly reduces its completeness.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The project proposes to monitor Model Prediction Latency under the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based on the application's requirements, CloudWatch alarms will be set up to notify the team if this limit is exceeded. Additionally, during the staging phase with general users, latency data will be collected under real-world conditions to understand typical latency patterns, allowing the team to set a realistic baseline and react swiftly if latency exceeds expected peaks, ensuring a seamless user experience.", "contexts": null, "response": "The project includes monitoring Model Prediction Latency using CloudWatch under the AWS/SageMaker namespace. An acceptable latency threshold will be established based on the application's needs, with alarms configured to alert the team if this threshold is surpassed. During the staging phase with general users, latency data will be gathered in real-world conditions to identify typical patterns. This will enable the team to set a realistic baseline and respond quickly if latency exceeds anticipated levels, thereby maintaining a smooth user experience.", "passing": true, "feedback": "The generated answer is relevant and accurately conveys the main points of the reference answer, but it uses slightly different wording and structure. It maintains the essential information and intent, making it a strong response overall.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are some key resources mentioned in the references that discuss content moderation and image classification?", "contexts": null, "response": "Some key resources that discuss content moderation and image classification include:\n\n1. **Amazon Web Services (2020)** - A guide on explaining image classification with SageMaker Clarify, which provides insights into image classification techniques.\n2. **Brown, R. (2023)** - An article discussing the importance of social media content moderation and how it operates.\n3. **Matheus Oliveira Franca (2021)** - A study on the detection and categorization of suggestive thumbnails, relevant to content moderation.\n4. **T.J. Thomson et al. (2022)** - An article highlighting the vast amount of images and videos shared online daily, emphasizing the challenges of sorting real from fake content.\n\nThese resources provide valuable information on the methodologies and implications of content moderation and image classification.", "passing": true, "feedback": "The generated answer is relevant and correctly identifies key resources related to content moderation and image classification. However, it introduces a new reference (T.J. Thomson et al. (2022)) that is not present in the reference answer, which slightly detracts from its accuracy. Overall, it maintains a high level of correctness and relevance.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The references include several key resources on content moderation and image classification, such as:", "contexts": null, "response": "The references include several key resources on content moderation and image classification, such as:\n\n1. GitHub repositories for NSFW data scraping and classifiers, including works by Alex000kim and NotAI.tech.\n2. Amazon Web Services documentation on image classification and moderation using SageMaker and Rekognition.\n3. Articles discussing the importance of social media content moderation and its implications for online platforms.\n4. Research papers on deep learning techniques, including the foundational work on ResNet and the Vision Transformer architecture.\n5. Various studies and reports highlighting the scale of content shared online and the challenges of moderating such content effectively.", "passing": false, "feedback": "The generated answer is relevant to the user query and provides a list of resources related to content moderation and image classification. However, it lacks specific details and links to the resources mentioned, which are present in the reference answer. This omission affects the completeness and correctness of the response.", "score": 3.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "\"Explaining Image Classification with SageMaker Clarify\" from Amazon Web Services, which discusses image classification techniques.", "contexts": null, "response": "The document discusses the use of SageMaker Clarify for image classification, emphasizing its role in enhancing model explainability. It highlights the importance of real-time monitoring of model explainability during both training and live data processing. By utilizing SHAP (SHapley Additive exPlanations) values, the system can provide insights into feature importance, indicating which parts of an image significantly influence predictions. This transparency is crucial for understanding model behavior and ensuring accountability in decision-making processes. The integration of such explainability tools aims to improve the robustness and trustworthiness of image classification systems.", "passing": true, "feedback": "The generated answer is relevant and provides a detailed explanation of the role of SageMaker Clarify in image classification, including the use of SHAP values for model explainability. However, it lacks a direct reference to the specific document or link mentioned in the reference answer, which would enhance its completeness.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "\"Why social media content moderation is important for online platforms & how it works?\" by Cogito Tech LLC, which emphasizes the significance of content moderation.", "contexts": null, "response": "Social media content moderation is crucial for online platforms as it helps maintain a safe and respectful environment for users. With the vast amount of content shared daily, including explicit and objectionable material, effective moderation is necessary to protect users from harmful content such as nudity, violence, and hate speech. \n\nModeration works through a combination of machine learning algorithms and human oversight. Platforms utilize advanced technologies to automatically identify and filter out inappropriate content, allowing for timely responses to violations. This process not only enhances user trust and safety but also helps uphold the brand reputation of the platforms. By implementing robust moderation systems, social media companies can foster a more responsible online community, ensuring that users can engage in a positive and secure manner.", "passing": true, "feedback": "The generated answer is relevant and covers the importance of social media content moderation effectively, touching on user safety, brand reputation, and the use of algorithms. However, it could be more concise and structured like the reference answer, which provides a clearer breakdown of points.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "The Amazon Rekognition Developer Guide, which provides information on moderating content using AWS tools.", "contexts": null, "response": "The Amazon Rekognition Developer Guide offers detailed information on how to utilize AWS tools for moderating content, including methods for detecting and classifying explicit imagery. It serves as a resource for understanding the capabilities of Amazon Rekognition in content moderation tasks, providing insights into the various categories and functionalities available for image and video analysis.", "passing": true, "feedback": "The generated answer is relevant and provides a detailed explanation of the Amazon Rekognition Developer Guide's purpose regarding content moderation. However, it lacks the specific link to the guide, which would enhance its completeness.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "The paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al., which presents advancements in image recognition technology.", "contexts": null, "response": "The paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al. introduces the ResNet architecture, which employs residual networks and skip connections to tackle the vanishing gradient problem, enabling the training of deeper neural networks. This innovation has significantly advanced image recognition technology, achieving state-of-the-art results at its inception.", "passing": true, "feedback": "The generated answer is relevant and accurately describes the main contributions of the paper, including the introduction of the ResNet architecture and its impact on image recognition technology. However, it lacks specific details such as the publication date and the authors' full names, which would enhance completeness.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What types of content are moderated on social media platforms according to the appendix of the report?", "contexts": null, "response": "The types of content moderated on social media platforms include explicit nudity, suggestive content, violence, vices, and racial slurs.", "passing": false, "feedback": "The generated answer is relevant to the user query as it lists types of content that are typically moderated on social media platforms. However, it does not reference the appendix of the report or Figure A, which is a key aspect of the reference answer. Additionally, the answer may not fully align with the specific content types mentioned in the report, leading to potential inaccuracies.", "score": 3.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The appendix includes a figure (Figure A) that outlines the types of content moderated on social media platforms, as referenced from Cognito.", "contexts": null, "response": "The appendix features a figure that details the various types of content that are moderated on social media platforms, sourced from Cognito.", "passing": false, "feedback": "The generated answer is relevant to the user query and accurately describes part of the content in the appendix, but it lacks the additional details provided in the reference answer, such as the mention of other figures and their contents.", "score": 3.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What does Figure D in the eyecatcher project report illustrate?", "contexts": null, "response": "Figure D illustrates the dataset statistics related to the eyecatcher project.", "passing": true, "feedback": "The generated answer is relevant and fully correct, accurately reflecting the content of the reference answer while maintaining clarity and specificity.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Figure D illustrates the dataset statistics related to the eyecatcher project.", "contexts": null, "response": "Figure D provides insights into the dataset statistics pertinent to the eyecatcher project.", "passing": true, "feedback": "The generated answer is relevant and conveys the same meaning as the reference answer, but it uses different wording and is slightly less direct.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the two figures mentioned in the context that relate to the model building process?", "contexts": null, "response": "The two figures related to the model building process are Figure F, which depicts the Detailed Architecture for Model Building, and Figure G, which illustrates the CodePipeline Stages for Model Building.", "passing": true, "feedback": "The generated answer is fully correct and matches the reference answer in both content and clarity, providing the exact figures and their descriptions as requested in the user query.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The two figures mentioned are Figure F, which illustrates the Detailed Architecture for Model Building, and Figure G, which outlines the CodePipeline Stages for Model Building.", "contexts": null, "response": "The figures referenced are Figure F, depicting the Detailed Architecture for Model Building, and Figure G, which presents the CodePipeline Stages for Model Building.", "passing": true, "feedback": "The generated answer is fully relevant and correct, matching the reference answer in content while using slightly different wording.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is depicted in Figure H of the eyecatcher project report?", "contexts": null, "response": "Figure H illustrates the detailed architecture for model deployment within the eyecatcher project.", "passing": true, "feedback": "The generated answer is relevant and fully correct, matching the reference answer in both content and clarity.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Figure H illustrates the detailed architecture for model deployment.", "contexts": null, "response": "Figure H provides a comprehensive overview of the architecture involved in the model deployment process.", "passing": true, "feedback": "The generated answer is relevant and conveys the same essential information as the reference answer, but it uses slightly different wording and adds the term \"comprehensive overview,\" which may imply a broader scope than intended.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the significance of the CodePipeline stages in the context of model deployment as illustrated in Figure I of the eyecatcher project report?", "contexts": null, "response": "The CodePipeline stages are crucial for ensuring a structured and efficient model deployment process. They facilitate the automation of various steps involved in deploying a machine learning model, from initial building and testing to final deployment. This structured approach helps in maintaining consistency, reducing errors, and enabling quick rollbacks if issues arise. By following these stages, the team can ensure that the model is thoroughly tested in a staging environment before being deployed to production, thereby enhancing the reliability and performance of the deployed model.", "passing": true, "feedback": "The generated answer is relevant and accurately describes the significance of the CodePipeline stages in model deployment, covering key aspects such as automation, consistency, and error reduction. However, it lacks some detail present in the reference answer regarding collaboration and tracking of changes, which prevents it from achieving a perfect score.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The CodePipeline stages are crucial for automating the deployment process of machine learning models, ensuring that each step\u2014from code integration to testing and deployment\u2014is systematically executed to maintain efficiency and reliability in the deployment workflow.", "contexts": null, "response": "The stages of CodePipeline play a vital role in automating the deployment of machine learning models. They ensure that every phase, from code integration to testing and deployment, is carried out in a systematic manner. This structured approach enhances both efficiency and reliability throughout the deployment workflow.", "passing": true, "feedback": "The generated answer is highly relevant and fully correct, closely mirroring the reference answer while maintaining clarity and conciseness.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is depicted in Figure K of the provided document?", "contexts": null, "response": "Figure K is not mentioned in the provided information, so its content cannot be determined.", "passing": false, "feedback": "The generated answer is not relevant to the user query as it fails to address what is depicted in Figure K and instead states that it cannot be determined, which is not helpful.", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Figure K illustrates an example of SageMaker Clarify, which is part of the Amazon SageMaker Examples.", "contexts": null, "response": "Figure K provides an example of SageMaker Clarify, showcasing its application within the Amazon SageMaker Examples.", "passing": true, "feedback": "The generated answer is relevant and conveys the same information as the reference answer, but it uses slightly different wording and adds the term \"showcasing its application,\" which may not be necessary.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the Word Error Rate (WER) score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files?", "contexts": null, "response": "The Word Error Rate (WER) score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.", "passing": true, "feedback": "The generated answer is fully correct and matches the reference answer exactly, providing the same WER score and context.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The WER score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.", "contexts": null, "response": "The WER score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.", "passing": true, "feedback": "The generated answer is identical to the reference answer, providing the same information accurately and concisely.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What strategies are proposed to improve the accuracy of the fine-tuned wav2vec2 model for speech recognition across diverse accents?", "contexts": null, "response": "To enhance the accuracy of the fine-tuned wav2vec2 model for speech recognition across various accents, several strategies are proposed:\n\n1. **Dataset Diversification and Augmentations**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection to create a more diverse training dataset. This aims to improve the model's generalizability to unique accents.\n\n2. **Integrating External Language Models**: Utilizing Large Language Models (LLMs) for post-processing to enhance performance. This involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction, which can help correct spelling mistakes and improve WER accuracy.\n\n3. **Hyperparameter Tuning**: Conducting hyperparameter tuning using methodologies like random search or Bayesian optimization to identify optimal settings for fine-tuning the model over a larger dataset.\n\n4. **Exploration of Other Methodologies**: Experimenting with semi-supervised learning strategies by utilizing self-transcribed, high-confidence data to supplement the training data pool, thereby potentially reducing WER further.\n\nThese multifaceted approaches aim to create a more robust and generalizable automatic speech recognition model capable of accurately transcribing speech across diverse linguistic and acoustic conditions.", "passing": true, "feedback": "The generated answer is relevant and covers the main strategies proposed in the reference answer, but it introduces an additional strategy (semi-supervised learning) that is not mentioned in the reference. While this addition is valuable, it slightly diverges from the original content, which affects the overall alignment with the reference answer.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The proposed strategies include:", "contexts": null, "response": "The proposed strategies include the use of a Canary deployment strategy for model rollout, which allows for incremental testing of new model versions with a controlled subset of users. This approach helps in monitoring performance, assessing risks, and detecting issues early. Additionally, an auto-scaling policy is implemented to ensure steady performance while managing costs, utilizing metrics to monitor invocation rates. Monitoring and retraining steps are also emphasized to handle concept and model drift, ensuring the model remains accurate and effective over time. Furthermore, robust verification processes are suggested to mitigate risks associated with adversarial attacks on the model.", "passing": false, "feedback": "The generated answer is relevant to the topic of proposed strategies but diverges significantly from the reference answer's focus on dataset diversification and model integration. It introduces different strategies related to deployment and monitoring, which may not align with the user's expectations based on the reference. While it contains relevant information, it does not address the specific strategies mentioned in the reference, leading to a lower score.", "score": 2.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Dataset Diversification and Augmentations**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection to create a more diverse training dataset.", "contexts": null, "response": "Implementing audio augmentation techniques can significantly enhance the diversity of the training dataset. Techniques such as speech perturbations introduce variations in the audio signals, which can help the model generalize better to different accents and speech patterns. Time masking allows for the omission of certain time segments in the audio, encouraging the model to focus on the remaining parts. Pitch shifting alters the frequency of the audio, providing additional variations that the model can learn from. Background noise injection simulates real-world conditions where speech may occur amidst various sounds, further enriching the training data. By employing these methods, the model can improve its performance and adaptability to diverse linguistic and acoustic conditions.", "passing": true, "feedback": "The generated answer is relevant and covers the main audio augmentation techniques mentioned in the reference answer, but it lacks some specific details and examples that would enhance its completeness.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Integrating External Language Models**: Leveraging Large Language Models (LLMs) for post-processing to enhance speech recognition accuracy by correcting context-aware errors and re-ranking outputs.", "contexts": null, "response": "Integrating Large Language Models (LLMs) for post-processing in speech recognition can significantly enhance accuracy. This approach involves using a transformer-based LLM decoder to perform next token prediction after the acoustic model has decoded the speech. By incorporating context-aware corrections, the model can address spelling mistakes that contribute to error rates in Word Error Rate (WER) metrics. Additionally, implementing an output re-ranking strategy can further refine the transcription results, leading to improved overall performance in speech recognition tasks.", "passing": true, "feedback": "The generated answer is relevant and accurately captures the essence of integrating LLMs for post-processing in speech recognition. It discusses key aspects such as next-token prediction, context-aware corrections, and output re-ranking. However, it lacks some depth and specific examples found in the reference answer, which could enhance clarity and understanding.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Hyperparameter Tuning**: Utilizing methodologies like random search or Bayesian optimization to find optimal hyperparameters for fine-tuning the model over a larger dataset (195,776 records) to improve performance.", "contexts": null, "response": "Hyperparameter tuning is a crucial step in optimizing model performance, especially when working with a larger dataset. Employing methodologies such as random search or Bayesian optimization can significantly enhance the fine-tuning process. Random search involves randomly selecting combinations of hyperparameters to evaluate, which can be more efficient than a grid search, especially in high-dimensional spaces. On the other hand, Bayesian optimization uses probabilistic models to identify the most promising hyperparameter configurations based on past evaluation results, allowing for a more informed search.\n\nBy applying these techniques to fine-tune the model over a dataset of 195,776 records, one can systematically explore the hyperparameter space to identify settings that yield the best performance. This approach not only improves the model's accuracy but also enhances its generalizability across different data distributions and conditions.", "passing": true, "feedback": "The generated answer is relevant and accurately describes hyperparameter tuning methodologies like random search and Bayesian optimization. It effectively addresses the user query by discussing the importance of these methods in optimizing model performance over a large dataset. However, it lacks some specific details present in the reference answer, such as the definition of hyperparameter space and the mention of evaluation metrics, which would enhance its completeness.", "score": 4.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What strategies are recommended to enhance the training data quality for model fine-tuning in speech recognition, as discussed in the provided context?", "contexts": null, "response": "To enhance the training data quality for model fine-tuning in speech recognition, several strategies are recommended:\n\n1. **Dataset Diversification and Augmentation**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection can create a more diverse training dataset, which is crucial for improving the model's generalizability to various accents.\n\n2. **Integrating External Language Models**: Leveraging large language models for post-processing can enhance performance. This involves using a transformer-based language model decoder to improve speech recognition accuracy through context-aware corrections and output re-ranking.\n\n3. **Hyperparameter Tuning**: Conducting hyperparameter tuning using methodologies like random search or Bayesian optimization can help identify optimal settings for fine-tuning the model.\n\n4. **Semi-Supervised Learning**: Utilizing self-transcribed, high-confidence data to supplement the training data pool can further enhance model fine-tuning. This involves selecting transcribed data based on model confidence levels, such as those with a Word Error Rate (WER) of 0.3 or lower.\n\nThese strategies aim to develop a more robust and generalizable automatic speech recognition model capable of accurately transcribing speech across diverse linguistic and acoustic conditions.", "passing": true, "feedback": "The generated answer is highly relevant and fully correct, closely aligning with the reference answer while providing additional details on audio augmentation techniques. It effectively covers all recommended strategies for enhancing training data quality in speech recognition.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The recommended strategies to enhance training data quality for model fine-tuning include dataset diversification, augmentation techniques, integration of external language models, hyperparameter tuning, and the use of semi-supervised learning strategies that leverage self-transcribed, high-confidence data to supplement the training data pool.", "contexts": null, "response": "To enhance training data quality for model fine-tuning, it is advisable to implement strategies such as diversifying the dataset, applying augmentation techniques, integrating external language models, tuning hyperparameters, and utilizing semi-supervised learning approaches that incorporate self-transcribed, high-confidence data to enrich the training data pool.", "passing": true, "feedback": "The generated answer is fully relevant and correct, matching the reference answer in content while being slightly more concise.", "score": 5.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}], "faithfulness": [{"query": "**Question:** Who are the members of Group 9 in the context of the eyecatcher project report?", "contexts": ["Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building"], "response": "The members of Group 9 in the context of the eyecatcher project report are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The members of Group 9 are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.", "contexts": ["Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)"], "response": "The members of Group 9 are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the key components of the machine learning system architecture as outlined in the project report?", "contexts": ["1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building"], "response": "The key components of the machine learning system architecture include:\n\n1. **Amazon S3 Training Bucket**: Stores training images converted into PyTorch Tensors for model training.\n2. **Amazon S3 Interim Bucket**: Stores reported or appealed images for moderators to evaluate.\n3. **Amazon SageMaker**: Used for model training, building, and compilation, as well as for model deployment.\n4. **AWS CodeCommit**: Stores source code and changes history.\n5. **AWS CodeBuild**: Compiles source code and builds the model.\n6. **AWS CodePipeline**: Automates the pipeline for continuous integration and continuous deployment (CI/CD).\n7. **Amazon CloudWatch**: Monitors model performance and logs, sending alarms as needed.\n8. **AWS Lambda**: Provides serverless computing for inference, updating data labels, and triggering model training pipelines.\n9. **API Gateway**: Facilitates interactions between public requests and AWS services. \n\nThese components work together to create a robust architecture for the machine learning system.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The key components of the machine learning system architecture outlined in the project report include model building, model deployment, and monitoring & retraining steps. Specifically, model building encompasses the workflow, data preprocessing, model training, and model quantization. Model deployment includes the deployment workflow, auto scaling policy, and deployment strategy. Finally, the monitoring & retraining step involves user feedback to handle concept and model drift, as well as the implementation of user feedback through a Discord server bot.", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The machine learning system architecture consists of several key components. Model building includes the workflow, data preprocessing, model training, and model quantization. For model deployment, it encompasses the deployment workflow, auto scaling policy, and deployment strategy. Lastly, the monitoring and retraining step focuses on user feedback to manage concept and model drift, along with the implementation of user feedback via a Discord server bot.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the primary focus of the project described in the report, and how does it aim to address the challenges of content moderation on social media platforms?", "contexts": ["11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "The primary focus of the project is to develop a deployable machine learning pipeline specifically for explicit image classification, with an emphasis on detecting explicit nudity. It aims to address the challenges of content moderation on social media platforms by providing a cost-effective tool that can accurately detect and filter out explicit images. This solution is designed to enhance user trust and safety while maintaining brand reputation. By leveraging advanced machine learning algorithms and cloud-native services, the project seeks to empower both users and community managers, enabling them to engage in content moderation effectively and efficiently. The system incorporates user feedback mechanisms to continuously improve its accuracy and adapt to changing content trends, thereby fostering a safer online environment.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The primary focus of the project is to develop a deployable machine learning pipeline for explicit image classification, specifically targeting explicit nudity detection. It aims to address the challenges of content moderation on social media platforms by providing a cost-effective tool that can accurately detect and filter out explicit images, thereby enhancing user trust and safety while maintaining brand reputation.", "contexts": ["2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "The project is centered on creating a machine learning pipeline designed for explicit image classification, with a specific emphasis on detecting explicit nudity. It seeks to tackle the difficulties associated with content moderation on social media platforms by offering an affordable solution that can effectively identify and filter explicit images. This initiative aims to bolster user trust and safety while also preserving the reputation of brands.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What method did the team use for data labeling in their explicit image classification project, and what were the advantages of this method?", "contexts": ["3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model."], "response": "The team used Amazon Rekognition's \"DetectModerationLabels\" method for data labeling in their explicit image classification project. The advantages of this method included improved data quality for training, validation, and testing while minimizing labeling costs. Additionally, since Rekognition utilizes an existing trained model for classification, it proved to be a more cost-effective solution compared to using human labelers through Amazon Ground Truth. This approach also allowed for the generation of multiple sub-classes/labels, enhancing the granularity of the dataset.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The team used Amazon Rekognition's \"DetectModerationLabels\" method for data labeling. The advantages of this method included improved quality of the dataset for training, validation, and testing while minimizing labeling costs. Additionally, it utilized an existing trained model for classification, making it more cost-effective compared to using human labelers through Amazon SageMaker Ground Truth.", "contexts": ["3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "The team opted for Amazon Rekognition's \"DetectModerationLabels\" method for data labeling due to its ability to enhance the quality of the dataset used for training, validation, and testing while also reducing labeling costs. This method leverages an existing trained model for classification, which provides a more economical solution compared to employing human labelers through Amazon SageMaker Ground Truth.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the main components of the machine learning system architecture described in the document, and what is the purpose of each component?", "contexts": ["8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below)."], "response": "The machine learning system architecture consists of several key components, each serving a specific purpose:\n\n1. **Amazon S3 Training Bucket**: This component stores training images that will be converted into PyTorch Tensors for model training.\n\n2. **Amazon S3 Interim Bucket**: It stores reported or appealed images for moderators to evaluate and take appropriate action.\n\n3. **Amazon SageMaker \u2013 Training**: This is used for conducting model training, building, and compilation.\n\n4. **Amazon SageMaker \u2013 Model Registry**: It contains a catalog of models to track and manage different versions of the models.\n\n5. **Amazon SageMaker - Endpoint**: This component is responsible for deploying the model for real-time inference.\n\n6. **AWS CodeCommit**: It stores the source code and maintains a history of changes made to it.\n\n7. **AWS CodeBuild**: This service compiles the source code and builds the model.\n\n8. **AWS CodePipeline**: It automates the pipeline for continuous integration and continuous deployment (CI/CD).\n\n9. **Amazon CloudWatch**: This component monitors model performance through logs and sends alarms based on predefined metrics.\n\n10. **AWS Lambda**: A serverless computing service that performs inference, updates data labels, and triggers the model training pipeline.\n\n11. **API Gateway**: This managed service facilitates interactions between public requests and AWS services.\n\nThese components work together to create a robust machine learning pipeline that supports model training, deployment, monitoring, and management.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The main components of the machine learning system architecture include:", "contexts": ["8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The main components of the machine learning system architecture include:\n\n1. **Amazon S3 Training Bucket** - Stores training images for model training.\n2. **Amazon S3 Interim Bucket** - Stores reported or appealed images for moderator evaluation.\n3. **Amazon SageMaker** - Used for model training, building, and compilation.\n4. **Amazon SageMaker Model Registry** - Contains a catalogue of models for tracking and management.\n5. **Amazon SageMaker Endpoint** - Deploys the model for real-time inference.\n6. **AWS CodeCommit** - Stores source code and change history.\n7. **AWS CodeBuild** - Compiles source code and builds the model.\n8. **AWS CodePipeline** - Automates the continuous integration and continuous deployment (CI/CD) pipeline.\n9. **Amazon CloudWatch** - Monitors model performance and sends alarms based on logs.\n10. **AWS Lambda** - Provides serverless computing for inference, data label updates, and triggering model training.\n11. **API Gateway** - Facilitates interactions between public requests and AWS services.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon S3 Training Bucket:** Stores training images that will be converted into PyTorch Tensor for model training.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "The Amazon S3 Training Bucket is utilized to store training images, which are subsequently converted into PyTorch Tensors for the purpose of model training.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon S3 Interim Bucket:** Stores reported/appealed images for moderators to evaluate and take appropriate action.", "contexts": ["3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "The Amazon S3 Interim Bucket is designated for storing images that have been reported or appealed. This allows moderators to evaluate these images and take the necessary actions based on their assessments.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon SageMaker \u2013 Training:** Conducts model training, building, and compilation.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below)."], "response": "Amazon SageMaker \u2013 Training is responsible for conducting the model training process, which includes building and compiling the model.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon SageMaker \u2013 Model Registry:** Contains a catalogue of models to track and manage.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "The Model Registry serves as a centralized repository for managing and tracking various models. It allows for organized oversight of model versions, facilitating easier updates and maintenance throughout the model lifecycle.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon SageMaker - Endpoint:** Deploys the model for real-time inference.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The Amazon SageMaker Endpoint is responsible for deploying the model, enabling it to perform real-time inference. This allows the model to generate predictions based on incoming data as it is received, facilitating immediate responses in applications that require timely decision-making.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**AWS CodeCommit:** Stores source code and changes history.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "15 \nFigure I: CodePipeline Stages for Model Deployment", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "14 \nFigure H: Detailed Architecture for Model Deployment", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building"], "response": "AWS CodeCommit is a service that is used to store source code and maintain a history of changes made to that code.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**AWS CodeBuild:** Compiles source code and builds the model.", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "15 \nFigure I: CodePipeline Stages for Model Deployment", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "14 \nFigure H: Detailed Architecture for Model Deployment", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "AWS CodeBuild is responsible for compiling the source code and building the model within the machine learning system architecture.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**AWS CodePipeline:** Automates the pipeline for Continuous Integration/Continuous Deployment (CI/CD).", "contexts": ["15 \nFigure I: CodePipeline Stages for Model Deployment", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify ."], "response": "AWS CodePipeline is a service that automates the pipeline for Continuous Integration and Continuous Deployment (CI/CD). It streamlines the process of building, testing, and deploying applications by integrating various stages of the development workflow, ensuring that code changes are automatically processed and delivered efficiently.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the key differences between the ResNet50 and Vision Transformer (ViT-16) models in terms of their architecture and performance based on the context provided?", "contexts": ["5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture"], "response": "The ResNet50 model is a deep convolutional neural network that utilizes residual networks and introduces skip connections to mitigate vanishing gradient issues, allowing for the training of deeper networks. Although it achieved state-of-the-art results at its inception, it has since been surpassed by other architectures. Its performance accuracy is around 20%.\n\nIn contrast, the Vision Transformer (ViT-16) employs a transformer architecture for image classification, processing images by dividing them into patches and using self-attention layers to facilitate long-range interactions between image elements. This model does not rely on convolutional layers, enabling it to capture global dependencies effectively. The ViT-16 model demonstrates a significantly higher performance accuracy of approximately 60%.\n\nOverall, the key differences lie in their architectural approaches\u2014ResNet50 focuses on convolutional layers with residual connections, while ViT-16 leverages transformer mechanisms\u2014and their performance, with ViT-16 outperforming ResNet50 in accuracy.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** ResNet50 is a deep convolutional neural network that utilizes residual networks and skip connections to address vanishing gradient problems, making it easier to train deeper networks. In contrast, Vision Transformer (ViT-16) employs a transformer architecture that processes images by dividing them into patches and using self-attention layers, allowing for long-range interactions between image elements and eliminating the need for convolutional layers. In terms of performance, ViT-16 significantly outperforms ResNet50, achieving an accuracy of approximately 60% compared to ResNet50's ~20%.", "contexts": ["5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms"], "response": "ResNet50 is a deep convolutional neural network that employs residual networks and skip connections to tackle vanishing gradient issues, facilitating the training of deeper networks. On the other hand, Vision Transformer (ViT-16) uses a transformer architecture to process images by segmenting them into patches and applying self-attention layers, which enables long-range interactions among image elements and removes the necessity for convolutional layers. In terms of model performance, ViT-16 demonstrates a significant advantage, achieving around 60% accuracy, while ResNet50 reaches approximately 20%.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the purpose of using a Canary deployment strategy in the context of the image classification model described in the report?", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The purpose of using a Canary deployment strategy for the image classification model is to enable an incremental rollout of new model versions to a small, controlled subset of users or environments before a full deployment. This approach allows for testing the model's performance, assessing potential risks, and identifying issues early without impacting the entire user base. It ensures a seamless experience for end-users by monitoring the new model's performance, measuring prediction accuracy, and detecting any unexpected behaviors or anomalies. Additionally, it facilitates a direct comparison between the new and existing model versions under real-world conditions, providing valuable feedback for decision-making regarding full deployment. The strategy also minimizes risks associated with deploying new models, allowing for quick rollbacks if problems arise, thereby protecting the overall system and user experience.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The purpose of using a Canary deployment strategy is to release new versions of the model to a small, controlled subset of users or environments before a full rollout. This approach allows the team to test the model's performance, assess potential risks, and detect issues early without affecting the entire user base. It enables monitoring of metrics such as prediction accuracy, model latency, throughput, and resource usage under real-world conditions, thereby reducing the risk associated with deploying new models and providing a safety net for high reliability and performance consistency.", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "15 \nFigure I: CodePipeline Stages for Model Deployment", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The Canary deployment strategy is designed to incrementally release new model versions to a limited group of users or environments prior to a complete rollout. This method facilitates performance testing, risk assessment, and early issue detection without impacting the entire user base. It allows for the monitoring of critical metrics such as prediction accuracy, model latency, throughput, and resource usage in real-world scenarios. By adopting this strategy, the team can minimize deployment risks and ensure a reliable and consistent performance of the model.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** How does the implementation of user feedback through the Discord server bot contribute to the model validation process in the image classification project?", "contexts": ["7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure."], "response": "The implementation of user feedback through the Discord server bot plays a crucial role in the model validation process by facilitating community engagement and providing insights into the model's performance. When users upload images, the bot processes these images and sends them to various AWS services for classification. If an image is misclassified, users can appeal the decision, prompting moderators to review and confirm the labels. This feedback loop allows for the identification of wrongly classified images, which are then moved into the training bucket to create a ground truth dataset.\n\nAdditionally, the bot's auto-moderation actions for NSFW content help maintain community standards while also collecting data on user interactions and model performance. By regularly moving unreported images to the training bucket and triggering model retraining when misclassifications exceed a certain threshold, the system adapts to changes in user preferences and improves its accuracy over time. This iterative process of validation and retraining ensures that the model remains effective and relevant in a dynamic environment.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The implementation of user feedback through the Discord server bot contributes to the model validation process by allowing users to interact with the model's predictions. When users upload images, the bot sends the images for classification and stores the results along with metadata. If images are wrongly classified, users can appeal or report them, prompting moderators to verify the labels. This feedback loop helps identify and mitigate drift in the model's performance. Images confirmed to be misclassified are moved to the training bucket to form the ground truth, and when the number of wrongly classified images exceeds a predefined threshold, it triggers the model training pipeline, thereby continuously improving the model's accuracy.", "contexts": ["7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "The user feedback mechanism implemented via the Discord server bot plays a crucial role in enhancing the model validation process. Users can upload images for classification, and the bot facilitates the submission of these images for analysis while also recording relevant metadata. In cases where images are misclassified, users have the option to appeal or report these instances, which leads to moderators reviewing and confirming the accuracy of the labels. This iterative feedback loop is essential for detecting and addressing any drift in the model's performance. Misclassified images that are verified are transferred to the training bucket to establish a reliable ground truth. Additionally, if the number of misclassifications surpasses a set threshold, it activates the model training pipeline, ensuring ongoing improvements in the model's accuracy.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the significance of monitoring CPU utilization in the context of the image classification model, and what threshold was set for the alarm?", "contexts": ["8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "Monitoring CPU utilization is crucial for understanding the computational demands of the image classification model. It provides insights that can lead to optimizations, such as reducing the model's computational load or upgrading to a larger instance type if necessary. This monitoring serves as an early warning system for potential disruptions due to insufficient computing power. The alarm threshold for CPU usage was set to more than an average of 70% over a 5-minute interval.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Monitoring CPU utilization is significant because it provides valuable insights into the computational demands of the model, allowing for timely optimizations or upgrades to ensure sufficient computing power. The alarm threshold was set to more than an average of 70% CPU usage over every 5-minute interval based on baseline percentages.", "contexts": ["8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "Monitoring CPU utilization is crucial as it offers insights into the model's computational demands, enabling timely optimizations or upgrades to maintain adequate computing power. An alarm threshold has been established at over 70% average CPU usage for every 5-minute interval, based on baseline percentages.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What measures are proposed to monitor and manage Model Prediction Latency in the deployment infrastructure of the project?", "contexts": ["9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "To monitor and manage Model Prediction Latency, the project plans to track the ModelLatency metric under the AWS/SageMaker namespace in CloudWatch. An acceptable threshold for latency will be defined based on the application's requirements, and CloudWatch alarms will be set up to notify the team if this limit is exceeded. This proactive approach aims to maintain the responsiveness of the service and ensure a seamless user experience. Additionally, during the staging phase with general users, latency data will be collected under real-world conditions to understand typical latency patterns, which will help in setting a realistic baseline for ModelLatency. The team will analyze trends in latency, considering both average latency and peak times, to ensure timely responses and effective management of user interactions.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The project proposes to monitor Model Prediction Latency under the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based on the application's requirements, CloudWatch alarms will be set up to notify the team if this limit is exceeded. Additionally, during the staging phase with general users, latency data will be collected under real-world conditions to understand typical latency patterns, allowing the team to set a realistic baseline and react swiftly if latency exceeds expected peaks, ensuring a seamless user experience.", "contexts": ["9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The project includes monitoring Model Prediction Latency using CloudWatch under the AWS/SageMaker namespace. An acceptable latency threshold will be established based on the application's needs, with alarms configured to alert the team if this threshold is surpassed. During the staging phase with general users, latency data will be gathered in real-world conditions to identify typical patterns. This will enable the team to set a realistic baseline and respond quickly if latency exceeds anticipated levels, thereby maintaining a smooth user experience.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are some key resources mentioned in the references that discuss content moderation and image classification?", "contexts": ["10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment."], "response": "Some key resources that discuss content moderation and image classification include:\n\n1. **Amazon Web Services (2020)** - A guide on explaining image classification with SageMaker Clarify, which provides insights into image classification techniques.\n2. **Brown, R. (2023)** - An article discussing the importance of social media content moderation and how it operates.\n3. **Matheus Oliveira Franca (2021)** - A study on the detection and categorization of suggestive thumbnails, relevant to content moderation.\n4. **T.J. Thomson et al. (2022)** - An article highlighting the vast amount of images and videos shared online daily, emphasizing the challenges of sorting real from fake content.\n\nThese resources provide valuable information on the methodologies and implications of content moderation and image classification.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The references include several key resources on content moderation and image classification, such as:", "contexts": ["3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment."], "response": "The references include several key resources on content moderation and image classification, such as:\n\n1. GitHub repositories for NSFW data scraping and classifiers, including works by Alex000kim and NotAI.tech.\n2. Amazon Web Services documentation on image classification and moderation using SageMaker and Rekognition.\n3. Articles discussing the importance of social media content moderation and its implications for online platforms.\n4. Research papers on deep learning techniques, including the foundational work on ResNet and the Vision Transformer architecture.\n5. Various studies and reports highlighting the scale of content shared online and the challenges of moderating such content effectively.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "\"Explaining Image Classification with SageMaker Clarify\" from Amazon Web Services, which discusses image classification techniques.", "contexts": ["10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model."], "response": "The document discusses the use of SageMaker Clarify for image classification, emphasizing its role in enhancing model explainability. It highlights the importance of real-time monitoring of model explainability during both training and live data processing. By utilizing SHAP (SHapley Additive exPlanations) values, the system can provide insights into feature importance, indicating which parts of an image significantly influence predictions. This transparency is crucial for understanding model behavior and ensuring accountability in decision-making processes. The integration of such explainability tools aims to improve the robustness and trustworthiness of image classification systems.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "\"Why social media content moderation is important for online platforms & how it works?\" by Cogito Tech LLC, which emphasizes the significance of content moderation.", "contexts": ["10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "Social media content moderation is crucial for online platforms as it helps maintain a safe and respectful environment for users. With the vast amount of content shared daily, including explicit and objectionable material, effective moderation is necessary to protect users from harmful content such as nudity, violence, and hate speech. \n\nModeration works through a combination of machine learning algorithms and human oversight. Platforms utilize advanced technologies to automatically identify and filter out inappropriate content, allowing for timely responses to violations. This process not only enhances user trust and safety but also helps uphold the brand reputation of the platforms. By implementing robust moderation systems, social media companies can foster a more responsible online community, ensuring that users can engage in a positive and secure manner.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "The Amazon Rekognition Developer Guide, which provides information on moderating content using AWS tools.", "contexts": ["11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model."], "response": "The Amazon Rekognition Developer Guide offers detailed information on how to utilize AWS tools for moderating content, including methods for detecting and classifying explicit imagery. It serves as a resource for understanding the capabilities of Amazon Rekognition in content moderation tasks, providing insights into the various categories and functionalities available for image and video analysis.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "The paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al., which presents advancements in image recognition technology.", "contexts": ["5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below)."], "response": "The paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al. introduces the ResNet architecture, which employs residual networks and skip connections to tackle the vanishing gradient problem, enabling the training of deeper neural networks. This innovation has significantly advanced image recognition technology, achieving state-of-the-art results at its inception.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What types of content are moderated on social media platforms according to the appendix of the report?", "contexts": ["11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "The types of content moderated on social media platforms include explicit nudity, suggestive content, violence, vices, and racial slurs.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The appendix includes a figure (Figure A) that outlines the types of content moderated on social media platforms, as referenced from Cognito.", "contexts": ["11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "The appendix features a figure that details the various types of content that are moderated on social media platforms, sourced from Cognito.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What does Figure D in the eyecatcher project report illustrate?", "contexts": ["12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "14 \nFigure H: Detailed Architecture for Model Deployment", "15 \nFigure I: CodePipeline Stages for Model Deployment"], "response": "Figure D illustrates the dataset statistics related to the eyecatcher project.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Figure D illustrates the dataset statistics related to the eyecatcher project.", "contexts": ["12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "14 \nFigure H: Detailed Architecture for Model Deployment"], "response": "Figure D provides insights into the dataset statistics pertinent to the eyecatcher project.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the two figures mentioned in the context that relate to the model building process?", "contexts": ["13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "15 \nFigure I: CodePipeline Stages for Model Deployment", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "The two figures related to the model building process are Figure F, which depicts the Detailed Architecture for Model Building, and Figure G, which illustrates the CodePipeline Stages for Model Building.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The two figures mentioned are Figure F, which illustrates the Detailed Architecture for Model Building, and Figure G, which outlines the CodePipeline Stages for Model Building.", "contexts": ["13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "15 \nFigure I: CodePipeline Stages for Model Deployment", "14 \nFigure H: Detailed Architecture for Model Deployment", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture"], "response": "The figures referenced are Figure F, depicting the Detailed Architecture for Model Building, and Figure G, which presents the CodePipeline Stages for Model Building.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is depicted in Figure H of the eyecatcher project report?", "contexts": ["12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "14 \nFigure H: Detailed Architecture for Model Deployment"], "response": "Figure H illustrates the detailed architecture for model deployment within the eyecatcher project.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Figure H illustrates the detailed architecture for model deployment.", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "15 \nFigure I: CodePipeline Stages for Model Deployment", "14 \nFigure H: Detailed Architecture for Model Deployment", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "Figure H provides a comprehensive overview of the architecture involved in the model deployment process.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the significance of the CodePipeline stages in the context of model deployment as illustrated in Figure I of the eyecatcher project report?", "contexts": ["15 \nFigure I: CodePipeline Stages for Model Deployment", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "14 \nFigure H: Detailed Architecture for Model Deployment", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture"], "response": "The CodePipeline stages are crucial for ensuring a structured and efficient model deployment process. They facilitate the automation of various steps involved in deploying a machine learning model, from initial building and testing to final deployment. This structured approach helps in maintaining consistency, reducing errors, and enabling quick rollbacks if issues arise. By following these stages, the team can ensure that the model is thoroughly tested in a staging environment before being deployed to production, thereby enhancing the reliability and performance of the deployed model.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The CodePipeline stages are crucial for automating the deployment process of machine learning models, ensuring that each step\u2014from code integration to testing and deployment\u2014is systematically executed to maintain efficiency and reliability in the deployment workflow.", "contexts": ["15 \nFigure I: CodePipeline Stages for Model Deployment", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The stages of CodePipeline play a vital role in automating the deployment of machine learning models. They ensure that every phase, from code integration to testing and deployment, is carried out in a systematic manner. This structured approach enhances both efficiency and reliability throughout the deployment workflow.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is depicted in Figure K of the provided document?", "contexts": ["12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "15 \nFigure I: CodePipeline Stages for Model Deployment", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "14 \nFigure H: Detailed Architecture for Model Deployment"], "response": "Figure K is not mentioned in the provided information, so its content cannot be determined.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Figure K illustrates an example of SageMaker Clarify, which is part of the Amazon SageMaker Examples.", "contexts": ["16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)"], "response": "Figure K provides an example of SageMaker Clarify, showcasing its application within the Amazon SageMaker Examples.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the Word Error Rate (WER) score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files?", "contexts": ["Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment."], "response": "The Word Error Rate (WER) score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The WER score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.", "contexts": ["Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify ."], "response": "The WER score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What strategies are proposed to improve the accuracy of the fine-tuned wav2vec2 model for speech recognition across diverse accents?", "contexts": ["Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure."], "response": "To enhance the accuracy of the fine-tuned wav2vec2 model for speech recognition across various accents, several strategies are proposed:\n\n1. **Dataset Diversification and Augmentations**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection to create a more diverse training dataset. This aims to improve the model's generalizability to unique accents.\n\n2. **Integrating External Language Models**: Utilizing Large Language Models (LLMs) for post-processing to enhance performance. This involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction, which can help correct spelling mistakes and improve WER accuracy.\n\n3. **Hyperparameter Tuning**: Conducting hyperparameter tuning using methodologies like random search or Bayesian optimization to identify optimal settings for fine-tuning the model over a larger dataset.\n\n4. **Exploration of Other Methodologies**: Experimenting with semi-supervised learning strategies by utilizing self-transcribed, high-confidence data to supplement the training data pool, thereby potentially reducing WER further.\n\nThese multifaceted approaches aim to create a more robust and generalizable automatic speech recognition model capable of accurately transcribing speech across diverse linguistic and acoustic conditions.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The proposed strategies include:", "contexts": ["1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)"], "response": "The proposed strategies include the use of a Canary deployment strategy for model rollout, which allows for incremental testing of new model versions with a controlled subset of users. This approach helps in monitoring performance, assessing risks, and detecting issues early. Additionally, an auto-scaling policy is implemented to ensure steady performance while managing costs, utilizing metrics to monitor invocation rates. Monitoring and retraining steps are also emphasized to handle concept and model drift, ensuring the model remains accurate and effective over time. Furthermore, robust verification processes are suggested to mitigate risks associated with adversarial attacks on the model.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Dataset Diversification and Augmentations**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection to create a more diverse training dataset.", "contexts": ["to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "Implementing audio augmentation techniques can significantly enhance the diversity of the training dataset. Techniques such as speech perturbations introduce variations in the audio signals, which can help the model generalize better to different accents and speech patterns. Time masking allows for the omission of certain time segments in the audio, encouraging the model to focus on the remaining parts. Pitch shifting alters the frequency of the audio, providing additional variations that the model can learn from. Background noise injection simulates real-world conditions where speech may occur amidst various sounds, further enriching the training data. By employing these methods, the model can improve its performance and adaptability to diverse linguistic and acoustic conditions.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Integrating External Language Models**: Leveraging Large Language Models (LLMs) for post-processing to enhance speech recognition accuracy by correcting context-aware errors and re-ranking outputs.", "contexts": ["Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "Integrating Large Language Models (LLMs) for post-processing in speech recognition can significantly enhance accuracy. This approach involves using a transformer-based LLM decoder to perform next token prediction after the acoustic model has decoded the speech. By incorporating context-aware corrections, the model can address spelling mistakes that contribute to error rates in Word Error Rate (WER) metrics. Additionally, implementing an output re-ranking strategy can further refine the transcription results, leading to improved overall performance in speech recognition tasks.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Hyperparameter Tuning**: Utilizing methodologies like random search or Bayesian optimization to find optimal hyperparameters for fine-tuning the model over a larger dataset (195,776 records) to improve performance.", "contexts": ["9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "Hyperparameter tuning is a crucial step in optimizing model performance, especially when working with a larger dataset. Employing methodologies such as random search or Bayesian optimization can significantly enhance the fine-tuning process. Random search involves randomly selecting combinations of hyperparameters to evaluate, which can be more efficient than a grid search, especially in high-dimensional spaces. On the other hand, Bayesian optimization uses probabilistic models to identify the most promising hyperparameter configurations based on past evaluation results, allowing for a more informed search.\n\nBy applying these techniques to fine-tune the model over a dataset of 195,776 records, one can systematically explore the hyperparameter space to identify settings that yield the best performance. This approach not only improves the model's accuracy but also enhances its generalizability across different data distributions and conditions.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What strategies are recommended to enhance the training data quality for model fine-tuning in speech recognition, as discussed in the provided context?", "contexts": ["to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "To enhance the training data quality for model fine-tuning in speech recognition, several strategies are recommended:\n\n1. **Dataset Diversification and Augmentation**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection can create a more diverse training dataset, which is crucial for improving the model's generalizability to various accents.\n\n2. **Integrating External Language Models**: Leveraging large language models for post-processing can enhance performance. This involves using a transformer-based language model decoder to improve speech recognition accuracy through context-aware corrections and output re-ranking.\n\n3. **Hyperparameter Tuning**: Conducting hyperparameter tuning using methodologies like random search or Bayesian optimization can help identify optimal settings for fine-tuning the model.\n\n4. **Semi-Supervised Learning**: Utilizing self-transcribed, high-confidence data to supplement the training data pool can further enhance model fine-tuning. This involves selecting transcribed data based on model confidence levels, such as those with a Word Error Rate (WER) of 0.3 or lower.\n\nThese strategies aim to develop a more robust and generalizable automatic speech recognition model capable of accurately transcribing speech across diverse linguistic and acoustic conditions.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The recommended strategies to enhance training data quality for model fine-tuning include dataset diversification, augmentation techniques, integration of external language models, hyperparameter tuning, and the use of semi-supervised learning strategies that leverage self-transcribed, high-confidence data to supplement the training data pool.", "contexts": ["to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "To enhance training data quality for model fine-tuning, it is advisable to implement strategies such as diversifying the dataset, applying augmentation techniques, integrating external language models, tuning hyperparameters, and utilizing semi-supervised learning approaches that incorporate self-transcribed, high-confidence data to enrich the training data pool.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}], "relevancy": [{"query": "**Question:** Who are the members of Group 9 in the context of the eyecatcher project report?", "contexts": ["Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building"], "response": "The members of Group 9 in the context of the eyecatcher project report are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The members of Group 9 are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.", "contexts": ["Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)"], "response": "The members of Group 9 are Christover Abraham Manafe, Loh Kwang Peng Micheal, Low Siang Leng Henry, and Yee Jin Kett.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the key components of the machine learning system architecture as outlined in the project report?", "contexts": ["1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building"], "response": "The key components of the machine learning system architecture include:\n\n1. **Amazon S3 Training Bucket**: Stores training images converted into PyTorch Tensors for model training.\n2. **Amazon S3 Interim Bucket**: Stores reported or appealed images for moderators to evaluate.\n3. **Amazon SageMaker**: Used for model training, building, and compilation, as well as for model deployment.\n4. **AWS CodeCommit**: Stores source code and changes history.\n5. **AWS CodeBuild**: Compiles source code and builds the model.\n6. **AWS CodePipeline**: Automates the pipeline for continuous integration and continuous deployment (CI/CD).\n7. **Amazon CloudWatch**: Monitors model performance and logs, sending alarms as needed.\n8. **AWS Lambda**: Provides serverless computing for inference, updating data labels, and triggering model training pipelines.\n9. **API Gateway**: Facilitates interactions between public requests and AWS services. \n\nThese components work together to create a robust architecture for the machine learning system.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The key components of the machine learning system architecture outlined in the project report include model building, model deployment, and monitoring & retraining steps. Specifically, model building encompasses the workflow, data preprocessing, model training, and model quantization. Model deployment includes the deployment workflow, auto scaling policy, and deployment strategy. Finally, the monitoring & retraining step involves user feedback to handle concept and model drift, as well as the implementation of user feedback through a Discord server bot.", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The machine learning system architecture consists of several key components. Model building includes the workflow, data preprocessing, model training, and model quantization. For model deployment, it encompasses the deployment workflow, auto scaling policy, and deployment strategy. Lastly, the monitoring and retraining step focuses on user feedback to manage concept and model drift, along with the implementation of user feedback via a Discord server bot.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the primary focus of the project described in the report, and how does it aim to address the challenges of content moderation on social media platforms?", "contexts": ["11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "The primary focus of the project is to develop a deployable machine learning pipeline specifically for explicit image classification, with an emphasis on detecting explicit nudity. It aims to address the challenges of content moderation on social media platforms by providing a cost-effective tool that can accurately detect and filter out explicit images. This solution is designed to enhance user trust and safety while maintaining brand reputation. By leveraging advanced machine learning algorithms and cloud-native services, the project seeks to empower both users and community managers, enabling them to engage in content moderation effectively and efficiently. The system incorporates user feedback mechanisms to continuously improve its accuracy and adapt to changing content trends, thereby fostering a safer online environment.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The primary focus of the project is to develop a deployable machine learning pipeline for explicit image classification, specifically targeting explicit nudity detection. It aims to address the challenges of content moderation on social media platforms by providing a cost-effective tool that can accurately detect and filter out explicit images, thereby enhancing user trust and safety while maintaining brand reputation.", "contexts": ["2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "The project is centered on creating a machine learning pipeline designed for explicit image classification, with a specific emphasis on detecting explicit nudity. It seeks to tackle the difficulties associated with content moderation on social media platforms by offering an affordable solution that can effectively identify and filter explicit images. This initiative aims to bolster user trust and safety while also preserving the reputation of brands.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What method did the team use for data labeling in their explicit image classification project, and what were the advantages of this method?", "contexts": ["3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model."], "response": "The team used Amazon Rekognition's \"DetectModerationLabels\" method for data labeling in their explicit image classification project. The advantages of this method included improved data quality for training, validation, and testing while minimizing labeling costs. Additionally, since Rekognition utilizes an existing trained model for classification, it proved to be a more cost-effective solution compared to using human labelers through Amazon Ground Truth. This approach also allowed for the generation of multiple sub-classes/labels, enhancing the granularity of the dataset.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The team used Amazon Rekognition's \"DetectModerationLabels\" method for data labeling. The advantages of this method included improved quality of the dataset for training, validation, and testing while minimizing labeling costs. Additionally, it utilized an existing trained model for classification, making it more cost-effective compared to using human labelers through Amazon SageMaker Ground Truth.", "contexts": ["3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "The team opted for Amazon Rekognition's \"DetectModerationLabels\" method for data labeling due to its ability to enhance the quality of the dataset used for training, validation, and testing while also reducing labeling costs. This method leverages an existing trained model for classification, which provides a more economical solution compared to employing human labelers through Amazon SageMaker Ground Truth.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the main components of the machine learning system architecture described in the document, and what is the purpose of each component?", "contexts": ["8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below)."], "response": "The machine learning system architecture consists of several key components, each serving a specific purpose:\n\n1. **Amazon S3 Training Bucket**: This component stores training images that will be converted into PyTorch Tensors for model training.\n\n2. **Amazon S3 Interim Bucket**: It stores reported or appealed images for moderators to evaluate and take appropriate action.\n\n3. **Amazon SageMaker \u2013 Training**: This is used for conducting model training, building, and compilation.\n\n4. **Amazon SageMaker \u2013 Model Registry**: It contains a catalog of models to track and manage different versions of the models.\n\n5. **Amazon SageMaker - Endpoint**: This component is responsible for deploying the model for real-time inference.\n\n6. **AWS CodeCommit**: It stores the source code and maintains a history of changes made to it.\n\n7. **AWS CodeBuild**: This service compiles the source code and builds the model.\n\n8. **AWS CodePipeline**: It automates the pipeline for continuous integration and continuous deployment (CI/CD).\n\n9. **Amazon CloudWatch**: This component monitors model performance through logs and sends alarms based on predefined metrics.\n\n10. **AWS Lambda**: A serverless computing service that performs inference, updates data labels, and triggers the model training pipeline.\n\n11. **API Gateway**: This managed service facilitates interactions between public requests and AWS services.\n\nThese components work together to create a robust machine learning pipeline that supports model training, deployment, monitoring, and management.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The main components of the machine learning system architecture include:", "contexts": ["8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The main components of the machine learning system architecture include:\n\n1. **Amazon S3 Training Bucket** - Stores training images for model training.\n2. **Amazon S3 Interim Bucket** - Stores reported or appealed images for moderator evaluation.\n3. **Amazon SageMaker** - Used for model training, building, and compilation.\n4. **Amazon SageMaker Model Registry** - Contains a catalogue of models for tracking and management.\n5. **Amazon SageMaker Endpoint** - Deploys the model for real-time inference.\n6. **AWS CodeCommit** - Stores source code and change history.\n7. **AWS CodeBuild** - Compiles source code and builds the model.\n8. **AWS CodePipeline** - Automates the continuous integration and continuous deployment (CI/CD) pipeline.\n9. **Amazon CloudWatch** - Monitors model performance and sends alarms based on logs.\n10. **AWS Lambda** - Provides serverless computing for inference, data label updates, and triggering model training.\n11. **API Gateway** - Facilitates interactions between public requests and AWS services.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon S3 Training Bucket:** Stores training images that will be converted into PyTorch Tensor for model training.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "The Amazon S3 Training Bucket is utilized to store training images, which are subsequently converted into PyTorch Tensors for the purpose of model training.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon S3 Interim Bucket:** Stores reported/appealed images for moderators to evaluate and take appropriate action.", "contexts": ["3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "The Amazon S3 Interim Bucket is designated for storing images that have been reported or appealed. This allows moderators to evaluate these images and take the necessary actions based on their assessments.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon SageMaker \u2013 Training:** Conducts model training, building, and compilation.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below)."], "response": "Amazon SageMaker \u2013 Training is responsible for conducting the model training process, which includes building and compiling the model.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon SageMaker \u2013 Model Registry:** Contains a catalogue of models to track and manage.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "The Model Registry serves as a centralized repository for managing and tracking various models. It allows for organized oversight of model versions, facilitating easier updates and maintenance throughout the model lifecycle.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Amazon SageMaker - Endpoint:** Deploys the model for real-time inference.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The Amazon SageMaker Endpoint is responsible for deploying the model, enabling it to perform real-time inference. This allows the model to generate predictions based on incoming data as it is received, facilitating immediate responses in applications that require timely decision-making.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**AWS CodeCommit:** Stores source code and changes history.", "contexts": ["4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "15 \nFigure I: CodePipeline Stages for Model Deployment", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "14 \nFigure H: Detailed Architecture for Model Deployment", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building"], "response": "AWS CodeCommit is a service that is used to store source code and maintain a history of changes made to that code.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**AWS CodeBuild:** Compiles source code and builds the model.", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "15 \nFigure I: CodePipeline Stages for Model Deployment", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "14 \nFigure H: Detailed Architecture for Model Deployment", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "AWS CodeBuild is responsible for compiling the source code and building the model within the machine learning system architecture.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**AWS CodePipeline:** Automates the pipeline for Continuous Integration/Continuous Deployment (CI/CD).", "contexts": ["15 \nFigure I: CodePipeline Stages for Model Deployment", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify ."], "response": "AWS CodePipeline is a service that automates the pipeline for Continuous Integration and Continuous Deployment (CI/CD). It streamlines the process of building, testing, and deploying applications by integrating various stages of the development workflow, ensuring that code changes are automatically processed and delivered efficiently.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the key differences between the ResNet50 and Vision Transformer (ViT-16) models in terms of their architecture and performance based on the context provided?", "contexts": ["5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture"], "response": "The ResNet50 model is a deep convolutional neural network that utilizes residual networks and introduces skip connections to mitigate vanishing gradient issues, allowing for the training of deeper networks. Although it achieved state-of-the-art results at its inception, it has since been surpassed by other architectures. Its performance accuracy is around 20%.\n\nIn contrast, the Vision Transformer (ViT-16) employs a transformer architecture for image classification, processing images by dividing them into patches and using self-attention layers to facilitate long-range interactions between image elements. This model does not rely on convolutional layers, enabling it to capture global dependencies effectively. The ViT-16 model demonstrates a significantly higher performance accuracy of approximately 60%.\n\nOverall, the key differences lie in their architectural approaches\u2014ResNet50 focuses on convolutional layers with residual connections, while ViT-16 leverages transformer mechanisms\u2014and their performance, with ViT-16 outperforming ResNet50 in accuracy.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** ResNet50 is a deep convolutional neural network that utilizes residual networks and skip connections to address vanishing gradient problems, making it easier to train deeper networks. In contrast, Vision Transformer (ViT-16) employs a transformer architecture that processes images by dividing them into patches and using self-attention layers, allowing for long-range interactions between image elements and eliminating the need for convolutional layers. In terms of performance, ViT-16 significantly outperforms ResNet50, achieving an accuracy of approximately 60% compared to ResNet50's ~20%.", "contexts": ["5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms"], "response": "ResNet50 is a deep convolutional neural network that employs residual networks and skip connections to tackle vanishing gradient issues, facilitating the training of deeper networks. On the other hand, Vision Transformer (ViT-16) uses a transformer architecture to process images by segmenting them into patches and applying self-attention layers, which enables long-range interactions among image elements and removes the necessity for convolutional layers. In terms of model performance, ViT-16 demonstrates a significant advantage, achieving around 60% accuracy, while ResNet50 reaches approximately 20%.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the purpose of using a Canary deployment strategy in the context of the image classification model described in the report?", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The purpose of using a Canary deployment strategy for the image classification model is to enable an incremental rollout of new model versions to a small, controlled subset of users or environments before a full deployment. This approach allows for testing the model's performance, assessing potential risks, and identifying issues early without impacting the entire user base. It ensures a seamless experience for end-users by monitoring the new model's performance, measuring prediction accuracy, and detecting any unexpected behaviors or anomalies. Additionally, it facilitates a direct comparison between the new and existing model versions under real-world conditions, providing valuable feedback for decision-making regarding full deployment. The strategy also minimizes risks associated with deploying new models, allowing for quick rollbacks if problems arise, thereby protecting the overall system and user experience.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The purpose of using a Canary deployment strategy is to release new versions of the model to a small, controlled subset of users or environments before a full rollout. This approach allows the team to test the model's performance, assess potential risks, and detect issues early without affecting the entire user base. It enables monitoring of metrics such as prediction accuracy, model latency, throughput, and resource usage under real-world conditions, thereby reducing the risk associated with deploying new models and providing a safety net for high reliability and performance consistency.", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "15 \nFigure I: CodePipeline Stages for Model Deployment", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The Canary deployment strategy is designed to incrementally release new model versions to a limited group of users or environments prior to a complete rollout. This method facilitates performance testing, risk assessment, and early issue detection without impacting the entire user base. It allows for the monitoring of critical metrics such as prediction accuracy, model latency, throughput, and resource usage in real-world scenarios. By adopting this strategy, the team can minimize deployment risks and ensure a reliable and consistent performance of the model.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** How does the implementation of user feedback through the Discord server bot contribute to the model validation process in the image classification project?", "contexts": ["7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure."], "response": "The implementation of user feedback through the Discord server bot plays a crucial role in the model validation process by facilitating community engagement and providing insights into the model's performance. When users upload images, the bot processes these images and sends them to various AWS services for classification. If an image is misclassified, users can appeal the decision, prompting moderators to review and confirm the labels. This feedback loop allows for the identification of wrongly classified images, which are then moved into the training bucket to create a ground truth dataset.\n\nAdditionally, the bot's auto-moderation actions for NSFW content help maintain community standards while also collecting data on user interactions and model performance. By regularly moving unreported images to the training bucket and triggering model retraining when misclassifications exceed a certain threshold, the system adapts to changes in user preferences and improves its accuracy over time. This iterative process of validation and retraining ensures that the model remains effective and relevant in a dynamic environment.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The implementation of user feedback through the Discord server bot contributes to the model validation process by allowing users to interact with the model's predictions. When users upload images, the bot sends the images for classification and stores the results along with metadata. If images are wrongly classified, users can appeal or report them, prompting moderators to verify the labels. This feedback loop helps identify and mitigate drift in the model's performance. Images confirmed to be misclassified are moved to the training bucket to form the ground truth, and when the number of wrongly classified images exceeds a predefined threshold, it triggers the model training pipeline, thereby continuously improving the model's accuracy.", "contexts": ["7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "The user feedback mechanism implemented via the Discord server bot plays a crucial role in enhancing the model validation process. Users can upload images for classification, and the bot facilitates the submission of these images for analysis while also recording relevant metadata. In cases where images are misclassified, users have the option to appeal or report these instances, which leads to moderators reviewing and confirming the accuracy of the labels. This iterative feedback loop is essential for detecting and addressing any drift in the model's performance. Misclassified images that are verified are transferred to the training bucket to establish a reliable ground truth. Additionally, if the number of misclassifications surpasses a set threshold, it activates the model training pipeline, ensuring ongoing improvements in the model's accuracy.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the significance of monitoring CPU utilization in the context of the image classification model, and what threshold was set for the alarm?", "contexts": ["8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "Monitoring CPU utilization is crucial for understanding the computational demands of the image classification model. It provides insights that can lead to optimizations, such as reducing the model's computational load or upgrading to a larger instance type if necessary. This monitoring serves as an early warning system for potential disruptions due to insufficient computing power. The alarm threshold for CPU usage was set to more than an average of 70% over a 5-minute interval.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Monitoring CPU utilization is significant because it provides valuable insights into the computational demands of the model, allowing for timely optimizations or upgrades to ensure sufficient computing power. The alarm threshold was set to more than an average of 70% CPU usage over every 5-minute interval based on baseline percentages.", "contexts": ["8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "Monitoring CPU utilization is crucial as it offers insights into the model's computational demands, enabling timely optimizations or upgrades to maintain adequate computing power. An alarm threshold has been established at over 70% average CPU usage for every 5-minute interval, based on baseline percentages.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What measures are proposed to monitor and manage Model Prediction Latency in the deployment infrastructure of the project?", "contexts": ["9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms."], "response": "To monitor and manage Model Prediction Latency, the project plans to track the ModelLatency metric under the AWS/SageMaker namespace in CloudWatch. An acceptable threshold for latency will be defined based on the application's requirements, and CloudWatch alarms will be set up to notify the team if this limit is exceeded. This proactive approach aims to maintain the responsiveness of the service and ensure a seamless user experience. Additionally, during the staging phase with general users, latency data will be collected under real-world conditions to understand typical latency patterns, which will help in setting a realistic baseline for ModelLatency. The team will analyze trends in latency, considering both average latency and peak times, to ensure timely responses and effective management of user interactions.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The project proposes to monitor Model Prediction Latency under the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based on the application's requirements, CloudWatch alarms will be set up to notify the team if this limit is exceeded. Additionally, during the staging phase with general users, latency data will be collected under real-world conditions to understand typical latency patterns, allowing the team to set a realistic baseline and react swiftly if latency exceeds expected peaks, ensuring a seamless user experience.", "contexts": ["9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The project includes monitoring Model Prediction Latency using CloudWatch under the AWS/SageMaker namespace. An acceptable latency threshold will be established based on the application's needs, with alarms configured to alert the team if this threshold is surpassed. During the staging phase with general users, latency data will be gathered in real-world conditions to identify typical patterns. This will enable the team to set a realistic baseline and respond quickly if latency exceeds anticipated levels, thereby maintaining a smooth user experience.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are some key resources mentioned in the references that discuss content moderation and image classification?", "contexts": ["10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment."], "response": "Some key resources that discuss content moderation and image classification include:\n\n1. **Amazon Web Services (2020)** - A guide on explaining image classification with SageMaker Clarify, which provides insights into image classification techniques.\n2. **Brown, R. (2023)** - An article discussing the importance of social media content moderation and how it operates.\n3. **Matheus Oliveira Franca (2021)** - A study on the detection and categorization of suggestive thumbnails, relevant to content moderation.\n4. **T.J. Thomson et al. (2022)** - An article highlighting the vast amount of images and videos shared online daily, emphasizing the challenges of sorting real from fake content.\n\nThese resources provide valuable information on the methodologies and implications of content moderation and image classification.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The references include several key resources on content moderation and image classification, such as:", "contexts": ["3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment."], "response": "The references include several key resources on content moderation and image classification, such as:\n\n1. GitHub repositories for NSFW data scraping and classifiers, including works by Alex000kim and NotAI.tech.\n2. Amazon Web Services documentation on image classification and moderation using SageMaker and Rekognition.\n3. Articles discussing the importance of social media content moderation and its implications for online platforms.\n4. Research papers on deep learning techniques, including the foundational work on ResNet and the Vision Transformer architecture.\n5. Various studies and reports highlighting the scale of content shared online and the challenges of moderating such content effectively.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "\"Explaining Image Classification with SageMaker Clarify\" from Amazon Web Services, which discusses image classification techniques.", "contexts": ["10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model."], "response": "The document discusses the use of SageMaker Clarify for image classification, emphasizing its role in enhancing model explainability. It highlights the importance of real-time monitoring of model explainability during both training and live data processing. By utilizing SHAP (SHapley Additive exPlanations) values, the system can provide insights into feature importance, indicating which parts of an image significantly influence predictions. This transparency is crucial for understanding model behavior and ensuring accountability in decision-making processes. The integration of such explainability tools aims to improve the robustness and trustworthiness of image classification systems.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "\"Why social media content moderation is important for online platforms & how it works?\" by Cogito Tech LLC, which emphasizes the significance of content moderation.", "contexts": ["10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "Social media content moderation is crucial for online platforms as it helps maintain a safe and respectful environment for users. With the vast amount of content shared daily, including explicit and objectionable material, effective moderation is necessary to protect users from harmful content such as nudity, violence, and hate speech. \n\nModeration works through a combination of machine learning algorithms and human oversight. Platforms utilize advanced technologies to automatically identify and filter out inappropriate content, allowing for timely responses to violations. This process not only enhances user trust and safety but also helps uphold the brand reputation of the platforms. By implementing robust moderation systems, social media companies can foster a more responsible online community, ensuring that users can engage in a positive and secure manner.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "The Amazon Rekognition Developer Guide, which provides information on moderating content using AWS tools.", "contexts": ["11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model."], "response": "The Amazon Rekognition Developer Guide offers detailed information on how to utilize AWS tools for moderating content, including methods for detecting and classifying explicit imagery. It serves as a resource for understanding the capabilities of Amazon Rekognition in content moderation tasks, providing insights into the various categories and functionalities available for image and video analysis.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "The paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al., which presents advancements in image recognition technology.", "contexts": ["5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below)."], "response": "The paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al. introduces the ResNet architecture, which employs residual networks and skip connections to tackle the vanishing gradient problem, enabling the training of deeper neural networks. This innovation has significantly advanced image recognition technology, achieving state-of-the-art results at its inception.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What types of content are moderated on social media platforms according to the appendix of the report?", "contexts": ["11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "The types of content moderated on social media platforms include explicit nudity, suggestive content, violence, vices, and racial slurs.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The appendix includes a figure (Figure A) that outlines the types of content moderated on social media platforms, as referenced from Cognito.", "contexts": ["11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "The appendix features a figure that details the various types of content that are moderated on social media platforms, sourced from Cognito.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What does Figure D in the eyecatcher project report illustrate?", "contexts": ["12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "14 \nFigure H: Detailed Architecture for Model Deployment", "15 \nFigure I: CodePipeline Stages for Model Deployment"], "response": "Figure D illustrates the dataset statistics related to the eyecatcher project.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Figure D illustrates the dataset statistics related to the eyecatcher project.", "contexts": ["12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "14 \nFigure H: Detailed Architecture for Model Deployment"], "response": "Figure D provides insights into the dataset statistics pertinent to the eyecatcher project.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What are the two figures mentioned in the context that relate to the model building process?", "contexts": ["13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "15 \nFigure I: CodePipeline Stages for Model Deployment", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "The two figures related to the model building process are Figure F, which depicts the Detailed Architecture for Model Building, and Figure G, which illustrates the CodePipeline Stages for Model Building.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The two figures mentioned are Figure F, which illustrates the Detailed Architecture for Model Building, and Figure G, which outlines the CodePipeline Stages for Model Building.", "contexts": ["13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "15 \nFigure I: CodePipeline Stages for Model Deployment", "14 \nFigure H: Detailed Architecture for Model Deployment", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture"], "response": "The figures referenced are Figure F, depicting the Detailed Architecture for Model Building, and Figure G, which presents the CodePipeline Stages for Model Building.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is depicted in Figure H of the eyecatcher project report?", "contexts": ["12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "14 \nFigure H: Detailed Architecture for Model Deployment"], "response": "Figure H illustrates the detailed architecture for model deployment within the eyecatcher project.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Figure H illustrates the detailed architecture for model deployment.", "contexts": ["6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "15 \nFigure I: CodePipeline Stages for Model Deployment", "14 \nFigure H: Detailed Architecture for Model Deployment", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "Figure H provides a comprehensive overview of the architecture involved in the model deployment process.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the significance of the CodePipeline stages in the context of model deployment as illustrated in Figure I of the eyecatcher project report?", "contexts": ["15 \nFigure I: CodePipeline Stages for Model Deployment", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "14 \nFigure H: Detailed Architecture for Model Deployment", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture"], "response": "The CodePipeline stages are crucial for ensuring a structured and efficient model deployment process. They facilitate the automation of various steps involved in deploying a machine learning model, from initial building and testing to final deployment. This structured approach helps in maintaining consistency, reducing errors, and enabling quick rollbacks if issues arise. By following these stages, the team can ensure that the model is thoroughly tested in a staging environment before being deployed to production, thereby enhancing the reliability and performance of the deployed model.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The CodePipeline stages are crucial for automating the deployment process of machine learning models, ensuring that each step\u2014from code integration to testing and deployment\u2014is systematically executed to maintain efficiency and reliability in the deployment workflow.", "contexts": ["15 \nFigure I: CodePipeline Stages for Model Deployment", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "The stages of CodePipeline play a vital role in automating the deployment of machine learning models. They ensure that every phase, from code integration to testing and deployment, is carried out in a systematic manner. This structured approach enhances both efficiency and reliability throughout the deployment workflow.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is depicted in Figure K of the provided document?", "contexts": ["12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "15 \nFigure I: CodePipeline Stages for Model Deployment", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "14 \nFigure H: Detailed Architecture for Model Deployment"], "response": "Figure K is not mentioned in the provided information, so its content cannot be determined.", "passing": false, "feedback": "NO", "score": 0.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** Figure K illustrates an example of SageMaker Clarify, which is part of the Amazon SageMaker Examples.", "contexts": ["16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)"], "response": "Figure K provides an example of SageMaker Clarify, showcasing its application within the Amazon SageMaker Examples.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What is the Word Error Rate (WER) score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files?", "contexts": ["Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment."], "response": "The Word Error Rate (WER) score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The WER score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.", "contexts": ["Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify ."], "response": "The WER score for the fine-tuned \"wav2vec2-large-960h\" model when evaluated on the cv-valid-test dataset with 6,300 files is 7.3%.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What strategies are proposed to improve the accuracy of the fine-tuned wav2vec2 model for speech recognition across diverse accents?", "contexts": ["Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure."], "response": "To enhance the accuracy of the fine-tuned wav2vec2 model for speech recognition across various accents, several strategies are proposed:\n\n1. **Dataset Diversification and Augmentations**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection to create a more diverse training dataset. This aims to improve the model's generalizability to unique accents.\n\n2. **Integrating External Language Models**: Utilizing Large Language Models (LLMs) for post-processing to enhance performance. This involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction, which can help correct spelling mistakes and improve WER accuracy.\n\n3. **Hyperparameter Tuning**: Conducting hyperparameter tuning using methodologies like random search or Bayesian optimization to identify optimal settings for fine-tuning the model over a larger dataset.\n\n4. **Exploration of Other Methodologies**: Experimenting with semi-supervised learning strategies by utilizing self-transcribed, high-confidence data to supplement the training data pool, thereby potentially reducing WER further.\n\nThese multifaceted approaches aim to create a more robust and generalizable automatic speech recognition model capable of accurately transcribing speech across diverse linguistic and acoustic conditions.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The proposed strategies include:", "contexts": ["1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)"], "response": "The proposed strategies include the use of a Canary deployment strategy for model rollout, which allows for incremental testing of new model versions with a controlled subset of users. This approach helps in monitoring performance, assessing risks, and detecting issues early. Additionally, an auto-scaling policy is implemented to ensure steady performance while managing costs, utilizing metrics to monitor invocation rates. Monitoring and retraining steps are also emphasized to handle concept and model drift, ensuring the model remains accurate and effective over time. Furthermore, robust verification processes are suggested to mitigate risks associated with adversarial attacks on the model.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Dataset Diversification and Augmentations**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection to create a more diverse training dataset.", "contexts": ["to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "Implementing audio augmentation techniques can significantly enhance the diversity of the training dataset. Techniques such as speech perturbations introduce variations in the audio signals, which can help the model generalize better to different accents and speech patterns. Time masking allows for the omission of certain time segments in the audio, encouraging the model to focus on the remaining parts. Pitch shifting alters the frequency of the audio, providing additional variations that the model can learn from. Background noise injection simulates real-world conditions where speech may occur amidst various sounds, further enriching the training data. By employing these methods, the model can improve its performance and adaptability to diverse linguistic and acoustic conditions.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Integrating External Language Models**: Leveraging Large Language Models (LLMs) for post-processing to enhance speech recognition accuracy by correcting context-aware errors and re-ranking outputs.", "contexts": ["Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630"], "response": "Integrating Large Language Models (LLMs) for post-processing in speech recognition can significantly enhance accuracy. This approach involves using a transformer-based LLM decoder to perform next token prediction after the acoustic model has decoded the speech. By incorporating context-aware corrections, the model can address spelling mistakes that contribute to error rates in Word Error Rate (WER) metrics. Additionally, implementing an output re-ranking strategy can further refine the transcription results, leading to improved overall performance in speech recognition tasks.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Hyperparameter Tuning**: Utilizing methodologies like random search or Bayesian optimization to find optimal hyperparameters for fine-tuning the model over a larger dataset (195,776 records) to improve performance.", "contexts": ["9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11"], "response": "Hyperparameter tuning is a crucial step in optimizing model performance, especially when working with a larger dataset. Employing methodologies such as random search or Bayesian optimization can significantly enhance the fine-tuning process. Random search involves randomly selecting combinations of hyperparameters to evaluate, which can be more efficient than a grid search, especially in high-dimensional spaces. On the other hand, Bayesian optimization uses probabilistic models to identify the most promising hyperparameter configurations based on past evaluation results, allowing for a more informed search.\n\nBy applying these techniques to fine-tune the model over a dataset of 195,776 records, one can systematically explore the hyperparameter space to identify settings that yield the best performance. This approach not only improves the model's accuracy but also enhances its generalizability across different data distributions and conditions.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Question:** What strategies are recommended to enhance the training data quality for model fine-tuning in speech recognition, as discussed in the provided context?", "contexts": ["to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "To enhance the training data quality for model fine-tuning in speech recognition, several strategies are recommended:\n\n1. **Dataset Diversification and Augmentation**: Implementing audio augmentation techniques such as speech perturbations, time masking, pitch shifting, and background noise injection can create a more diverse training dataset, which is crucial for improving the model's generalizability to various accents.\n\n2. **Integrating External Language Models**: Leveraging large language models for post-processing can enhance performance. This involves using a transformer-based language model decoder to improve speech recognition accuracy through context-aware corrections and output re-ranking.\n\n3. **Hyperparameter Tuning**: Conducting hyperparameter tuning using methodologies like random search or Bayesian optimization can help identify optimal settings for fine-tuning the model.\n\n4. **Semi-Supervised Learning**: Utilizing self-transcribed, high-confidence data to supplement the training data pool can further enhance model fine-tuning. This involves selecting transcribed data based on model confidence levels, such as those with a Word Error Rate (WER) of 0.3 or lower.\n\nThese strategies aim to develop a more robust and generalizable automatic speech recognition model capable of accurately transcribing speech across diverse linguistic and acoustic conditions.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}, {"query": "**Answer:** The recommended strategies to enhance training data quality for model fine-tuning include dataset diversification, augmentation techniques, integration of external language models, hyperparameter tuning, and the use of semi-supervised learning strategies that leverage self-transcribed, high-confidence data to supplement the training data pool.", "contexts": ["to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building."], "response": "To enhance training data quality for model fine-tuning, it is advisable to implement strategies such as diversifying the dataset, applying augmentation techniques, integrating external language models, tuning hyperparameters, and utilizing semi-supervised learning approaches that incorporate self-transcribed, high-confidence data to enrich the training data pool.", "passing": true, "feedback": "YES", "score": 1.0, "pairwise_source": null, "invalid_result": false, "invalid_reason": null}]}