{"docstore/metadata": {"fc671f47-45ea-43d2-8eaf-c8e6c19a0666": {"doc_hash": "a6abbd4915a11980f3a00e46cf82113ebba362398888859073d5a4f8cd4a62cf"}, "9166a5ad-9f8c-4239-961c-9b1213e03535": {"doc_hash": "9770daab20f77b5f03f494a56425e6086e5675bf5eb3849134cf580894295776"}, "a4c6fa81-4dce-48b9-8c79-868b4ce4aef1": {"doc_hash": "5951f57540c7796f0a7194da27d3a7c276bad0d95e8111c865d61189359e860e"}, "d9adf0d0-964e-4de9-8b2f-ef118bf98890": {"doc_hash": "45f746e8d6c4beaee6a5d6d7dfc485445644b64d90ec0cb19c6ef8aafe2c4ca7"}, "c58f3433-543d-4a76-89e9-6c5cd909c7ce": {"doc_hash": "887e0e8b3d2f410448951865d71310a85fd71de38b53dbb79969cffb0f135e6d"}, "e783808c-8ace-4997-bec7-0c5c47ec10c8": {"doc_hash": "691652a8c6c79f4b21e58822b629180e21eba357bd883ef86eade3f54736a73a"}, "3611965f-3d94-46d6-991c-3f9797533747": {"doc_hash": "9f11adb6e347f7142e9291392d1378464875da500140366aab737cdd16935547"}, "bc9fabfe-5bca-4c51-acd4-e4f7c1a404a9": {"doc_hash": "78f76bca3ccb2fbad172070d0943b01bd2a60fb9d0d13d0ca8e7c6fb70907e6d"}, "b15423d3-f473-4e7a-82ff-27423f1df479": {"doc_hash": "fcd3fa019549018ec14117d2dd150977ce2f70078d979da7688d088da011264a"}, "eecb45a3-4c28-4a4d-a132-96c183ea2b89": {"doc_hash": "e685a4cdecfb6a44f8aaae9a2e1badab227375d0a82f444227786c9b5834b004"}, "ced3bcf8-e511-427b-842b-6c6e3951987b": {"doc_hash": "9f14d115d7b9862f5124500409bb382ee96d2d43cfb1035e9f12df95f11096bd"}, "1d96344b-702e-44ae-a628-c3def64cb9d4": {"doc_hash": "531455696fd78a41ee3508e516d800a787cbf57f6851a6e13f39b77761f106a2"}, "292ae60f-2ad5-45f6-a48a-dc1d6723ecf0": {"doc_hash": "d78ff4143109b715a6dd7fc06bdb1843d085d1aac8a36a385c32f4e1d104211e"}, "4edeab6d-1648-4e23-bde3-5cd9d208a703": {"doc_hash": "19b9769b99e29f8c7879bdedd702652334fedc42c415b42a179e2ca772f35ffc"}, "d14df1ff-70a0-4665-98b9-13fbef3395a4": {"doc_hash": "9d6c3cbc44077e0a884897188694e9dcda7b78345532172cf1b8476b029e6ed8"}, "e1de5b08-9c96-4658-86ca-c5324d408399": {"doc_hash": "a744cd5d0b86d781fb64b2d89cb120f0089f467356bc1823dec4692311df5bf3"}, "bea8a6f9-1886-4aae-9e6f-89175ae327de": {"doc_hash": "b112ab41dc42e0ef798a50b6b41134f35596d78cf0cdf81dc8bc9eb7170b81f6"}, "bb63d3cf-e16b-4268-8f91-f66dfea2927b": {"doc_hash": "e338adc54fb7a989945b2f31c87551ec7d1be4cf9f2ee106a2f9702b55cb988f"}, "16714dde-21c4-4c0b-9932-e3adf500b4fb": {"doc_hash": "17884dbe2acc78cbbe847b3cbc74860ecb76a8c101dc14a0872aebcbb4bc8dac"}, "c1e3138d-d435-4dc6-bfce-1af32afb4d57": {"doc_hash": "0d05648ebfa495e11cbb458d23b88530e21c59efbc64171eca47ec70e08ba17d"}, "da39b81f-f134-49e1-8cc4-cd8d4b7f0846": {"doc_hash": "8b99cc9ba94e37c629bb61063ce5261321be0ecf76cd8d04aad9cd5c758cfb50", "ref_doc_id": "fc671f47-45ea-43d2-8eaf-c8e6c19a0666"}, "8ac28684-2a61-489b-ae1b-c31b0465e84f": {"doc_hash": "69ebe8af9528aee446410552ef4093299f91d01251edcc987b7ea6eb4c8702b0", "ref_doc_id": "9166a5ad-9f8c-4239-961c-9b1213e03535"}, "4f77933c-ea9b-40d5-a673-0d8a5798b7a1": {"doc_hash": "1cf8f3d04240ee2b292bc55e310923dd6eba6022b5c33747678650d014be4e63", "ref_doc_id": "a4c6fa81-4dce-48b9-8c79-868b4ce4aef1"}, "9fca5882-e3b1-4199-99a5-1abf784622b9": {"doc_hash": "454d75922b513469b280b000d5a92c8ad52e5ca1bd2fb634227e637d9d4271d3", "ref_doc_id": "d9adf0d0-964e-4de9-8b2f-ef118bf98890"}, "37e23a5b-5813-4d7a-9b22-685a34bc8ee3": {"doc_hash": "1fe2d768109179d052786cf917a64804de64667f6b4a953afc141e359cbfde86", "ref_doc_id": "c58f3433-543d-4a76-89e9-6c5cd909c7ce"}, "3130f783-22c5-4215-a5b5-5afd1f4a427e": {"doc_hash": "c300d9ff2272d3004c328187984a31a45afef8d5d8ae79954adf3ea59205ca32", "ref_doc_id": "e783808c-8ace-4997-bec7-0c5c47ec10c8"}, "07c964c0-42f7-488c-aea9-20a4cc7221bd": {"doc_hash": "a5df6cfcf828f96ad21271e24355bcec25cd70eeb9784d1afc399f002d62d0d0", "ref_doc_id": "3611965f-3d94-46d6-991c-3f9797533747"}, "38ebd667-7939-4ac9-8a8b-42906bb73769": {"doc_hash": "09f7b3b171d6c1e46c9c8c036fd9eb3727fe32cc8e6942a793d30a0913291fb8", "ref_doc_id": "bc9fabfe-5bca-4c51-acd4-e4f7c1a404a9"}, "fb9e84fe-3085-490d-9784-7a511c57a769": {"doc_hash": "1a85890808365bab0e8d19f56b6096cc7dcb1917c1c79315341d92df8e7a0235", "ref_doc_id": "b15423d3-f473-4e7a-82ff-27423f1df479"}, "f9a61d15-4f33-45b3-b0f6-536e7b3ca5ba": {"doc_hash": "a1f671c2ba0ffcf27eb40e7e1a0ed8ba6eedcd91f6b7b2f586f2ebe4d1df1df7", "ref_doc_id": "eecb45a3-4c28-4a4d-a132-96c183ea2b89"}, "548a4ff0-bc4b-4138-9f67-0de7c8c8fd61": {"doc_hash": "21baac467d3b1079a969fe44f6decd1bab999e8b73726c386b9d0f38b6188030", "ref_doc_id": "ced3bcf8-e511-427b-842b-6c6e3951987b"}, "8fb81d93-cca7-420e-9756-5c237cf0ce4b": {"doc_hash": "7a02b8b441a4509c0c0ec3204a3db90367cf7bc61270761054cd64d51fee9bf1", "ref_doc_id": "1d96344b-702e-44ae-a628-c3def64cb9d4"}, "30b20683-4038-4104-b233-ec5b5e278f68": {"doc_hash": "5491ac338fe3fb4654208590df72322f8789af5e1d21281ecbfc4e08004fe198", "ref_doc_id": "292ae60f-2ad5-45f6-a48a-dc1d6723ecf0"}, "b693e1bb-78e2-4fd6-ada4-1cae0a81366d": {"doc_hash": "342c039f2ef7561f2429ee3a3127ea405d5db98f351278aef169612dc50cb72c", "ref_doc_id": "4edeab6d-1648-4e23-bde3-5cd9d208a703"}, "ae07d45d-345f-43fe-9c29-d0c21af04a44": {"doc_hash": "16ec88156c3070d4aea6e98c3abec871d2a137a5f6d3eae3e22b9a8766b9cf0f", "ref_doc_id": "d14df1ff-70a0-4665-98b9-13fbef3395a4"}, "e8b9906b-8855-436e-bb9d-1c7d00641d2d": {"doc_hash": "faa4977b439c2215610f1eb7e92002494af83b8057a5d150a1a0c43fc19dbfd6", "ref_doc_id": "e1de5b08-9c96-4658-86ca-c5324d408399"}, "4b826ae5-0c1d-4553-afef-104ff899f25a": {"doc_hash": "23a255f7f44c1b63a41ca395b4a06688883e5bdf2cccee9fc87ac1ec0d935df0", "ref_doc_id": "bea8a6f9-1886-4aae-9e6f-89175ae327de"}, "d04865cb-39b7-4ddc-8075-b9968c66137d": {"doc_hash": "0d4ddbcdf4149b78a66691682547a5cd43a3a5e6bfe4a2d8091c88a7e5f8ab5d", "ref_doc_id": "bb63d3cf-e16b-4268-8f91-f66dfea2927b"}, "de812c97-885e-4611-959b-b8a85acd75ea": {"doc_hash": "49623c1002d2d9f663a73f3c5e6b97e445bbc3e116b95fc8c505d3345b0bdce9", "ref_doc_id": "16714dde-21c4-4c0b-9932-e3adf500b4fb"}, "3885cd0a-0f92-4f48-916b-331c55cb9f83": {"doc_hash": "573b2ea7f9c094573034616d4fc43d761aeb540140565911a3fdefc1b78cb047", "ref_doc_id": "c1e3138d-d435-4dc6-bfce-1af32afb4d57"}}, "docstore/data": {"da39b81f-f134-49e1-8cc4-cd8d4b7f0846": {"__data__": {"id_": "da39b81f-f134-49e1-8cc4-cd8d4b7f0846", "embedding": null, "metadata": {"page_label": "1", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc671f47-45ea-43d2-8eaf-c8e6c19a0666", "node_type": "4", "metadata": {"page_label": "1", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "a6abbd4915a11980f3a00e46cf82113ebba362398888859073d5a4f8cd4a62cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Group 9 \nChristover Abraham Manafe \nLoh Kwang Peng Micheal \nLow Siang Leng Henry \nYee Jin Kett \nAEYECATCHER.PY \nCS611 - Machine Learning Engineering", "mimetype": "text/plain", "start_char_idx": 7, "end_char_idx": 155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8ac28684-2a61-489b-ae1b-c31b0465e84f": {"__data__": {"id_": "8ac28684-2a61-489b-ae1b-c31b0465e84f", "embedding": null, "metadata": {"page_label": "2", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9166a5ad-9f8c-4239-961c-9b1213e03535", "node_type": "4", "metadata": {"page_label": "2", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "9770daab20f77b5f03f494a56425e6086e5675bf5eb3849134cf580894295776", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 \nTable of Contents \n1. BUSINESS PROBLEM & IMPLICATIONS ......................................................................................................... 2 \n1.1. PROBLEM BACKGROUND ................................................................................................................................. 2 \n1.2. PROJECT FOCUS ............................................................................................................................................. 2 \n1.3. PROJECT SCOPE ............................................................................................................................................. 2 \n1.4. STAKEHOLDERS \u2013 USERS & ADMINISTRATORS ...................................................................................................... 2 \n2. DATA COLLECTION & PROJECT DATASETS .................................................................................................... 3 \n2.1. DATA SOURCES ............................................................................................................................................. 3 \n2.2. DATA LABELLING............................................................................................................................................ 3 \n2.3. DATASET STATISTICS ....................................................................................................................................... 3 \n2.4. DATA IMBALANCE .......................................................................................................................................... 3 \n2.5. DATASET FORMAT .......................................................................................................................................... 3 \n2.6. DATA PRIVACY & ETHICAL CONSIDERATIONS ........................................................................................................ 3 \n3. MACHINE LEARNING SYSTEM ARCHITECTURE .............................................................................................. 4 \n3.1. MODEL BUILDING .......................................................................................................................................... 4 \n3.1.1. MODEL BUILDING WORKFLOW............................................................................................................................ 4 \n3.1.2. DATA PREPROCESSING ....................................................................................................................................... 4 \n3.1.3. MODEL TRAINING ............................................................................................................................................. 5 \n3.1.4. MODEL QUANTIZATION ..................................................................................................................................... 5 \n3.2. MODEL DEPLOYMENT ..................................................................................................................................... 5 \n3.2.1. MODEL DEPLOYMENT WORKFLOW ...................................................................................................................... 5 \n3.2.2. AUTO SCALING POLICY ....................................................................................................................................... 6 \n3.2.3. DEPLOYMENT STRATEGY .................................................................................................................................... 6 \n3.3. MONITORING & RETRAINING STEP .................................................................................................................... 6 \n3.3.1. USER FEEDBACK TO HANDLE CONCEPT & MODEL DRIFT ........................................................................................... 6 \n3.3.2. IMPLEMENTATION OF USER FEEDBACK \u2013 DISCORD SERVER BOT ................................................................................ 7 \n4. LIMITATIONS, CONSIDERATIONS & FUTURE WORKS .................................................................................... 8 \n4.1. TECHNICAL LIMITATIONS .................................................................................................................................. 8 \n4.2. DATA LIMITATIONS ........................................................................................................................................ 8 \n4.3. MODEL LIMITATIONS ...................................................................................................................................... 8 \n4.4. DEPLOYMENT INFRASTRUCTURE ........................................................................................................................ 9 \n4.5. ETHICAL & LEGAL CONSIDERATIONS ................................................................................................................... 9 \n4.6. SCOPE EXPANSION ......................................................................................................................................... 9 \n5. REFERENCES ............................................................................................................................................... 10 \n6. APPENDIX .................................................................................................................................................. 11", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 5362, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4f77933c-ea9b-40d5-a673-0d8a5798b7a1": {"__data__": {"id_": "4f77933c-ea9b-40d5-a673-0d8a5798b7a1", "embedding": null, "metadata": {"page_label": "3", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4c6fa81-4dce-48b9-8c79-868b4ce4aef1", "node_type": "4", "metadata": {"page_label": "3", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "5951f57540c7796f0a7194da27d3a7c276bad0d95e8111c865d61189359e860e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 \n1. Business Problem & Implications  \n1.1. Problem Background \nIn today's digital age, social media platforms and website s have become an integral part of our lives, and the \namount of content being shared and uploaded online is increasing exponentially. With the rise in popularity of \nsocial media platforms like TikTok, Instagram, and Facebook, the need for explicit/NSFW1 image moderation has \nbecome more important than ever. With 3.2 billion images and 720,000 videos being shared daily (T.J. Thomson \net.al, 2022), this has given rise to the complexity of content moderation. Content moderation is an industry-wide \nproblem as cited by TikTok CEO Chew Shou Zi, and it is tough to identify and take down objectionable contents2 \nsuch as suggestive content, violence, vices & racial slurs in a timely manner. \nWhile social m edia giants like Facebook and TikTok have invested heavily in machi ne learning and human \nmoderators to conduct moderation activity to remove unsafe content from their respective platforms, start-ups \nand SMEs are unable to employ the similar processes due to budgetary constraints. \n1.2. Project Focus \nOur project aims to val ue add to this field by develop ing a deployable machine learning pipeline for explicit \nimage classification, with a particular focus on explicit nudity detection.  \n1.3. Project Scope \nWe pla n to use state -of-the-art machine learning algorithms and technique s to develop a model that can \naccurately detect and filter out explicit images, including, but not limited to nudity and sexual exposure. Through \nthis, businesses can leverage on a powerful  yet cost-effective tool to moderate the content on their platforms , \nenabling users\u2019 trust and safety while maintaining brand reputation. \nSubsequently, we would develop a cloud-native solution by leveragin g on services such as Amazon SageMaker \nand AWS Lambda that is highly tailored to the business\u2019 needs.  \n1.4. Stakeholders \u2013 Users & Administrators \nStakeholders, including both users and administrators, can leverage our machine learning system in various ways \nto enhance their experience and ensure a safer online environment. \nUsers of social media platforms will upload images a nd receive feedback on their  contents from the pipeline. \nThis feedback will indicate if the image contains explicit nudity or not. A dditionally, users can perform self -\nlabelling by repor ting inappropriate images (in situations where the ML system fail to f lag out inappropriate \nimages). When a certain threshold of reported images is reached, the system will trigger a model retraining to  \nimprove the accuracy of the pipeline's explicit image classification over time.  \nOn the other hand, social media community managers will be the primary  administrators of our machine \nlearning system. They will be responsible for maintaining the pipeline's functionality and ensuring the accuracy \nand reliability of the system. As part of their role, they will monitor the pipeline 's performance, fine -tune the \nsystem parameters, and carry out periodic updates to the model. By utilizing our ML system, administrators can \nfocus their efforts on managing the platform and creating a seamless user experience, while having confidence \nin the system's ability to enhance content moderation and foster a safer online community. \nExisting use-cases suggest that community mana gers often face the challenge of moderating user -generated \ncontent in real-time. To tackle this challenge, some companies ha ve implemented machine learning systems to \nhelp identify inappropriate content and flag them for review. Our machine learning system  aims to provide a \nsimilar solution that can effective ly support social media community managers in monitoring user -generated \ncontent for explicit nudity. By leveraging self-labelling features, the system can also promote user engagement \nand foster a safer online community. Overall, our ML system offers stakeholders a comprehensive solution that \nfacilitates content moderation, empowers user engagement, an d ultimately contributes to a more responsible \nand respectful online environment. \n \n1 Not safe for work \n2 See Appendix: Figure A for common categories of content moderation on Social Media platforms", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 4264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9fca5882-e3b1-4199-99a5-1abf784622b9": {"__data__": {"id_": "9fca5882-e3b1-4199-99a5-1abf784622b9", "embedding": null, "metadata": {"page_label": "4", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9adf0d0-964e-4de9-8b2f-ef118bf98890", "node_type": "4", "metadata": {"page_label": "4", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "45f746e8d6c4beaee6a5d6d7dfc485445644b64d90ec0cb19c6ef8aafe2c4ca7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3 \n2. Data Collection & Project Datasets \n2.1. Data Sources \nIn our data collecti on process3, we evaluated different options to obtain the necessary dataset for our explicit \nimage classification project. While one of the feasible options was to use Amazon SageMaker Ground Truth to \nlabel web scraped images from know n explicit sites and Google safe search images, we ultimately decided to \nleverage existing pre -labelled datasets, revie w and consolidate the images, and use Amazon Rekognition's \n\"DetectModerationLabels\" method as our labelling tool to generate multiple sub -classes/labels to improve the \ngranularity of our dataset. This approach allowed us to improve the quality of the data we use for training, \nvalidation, and testing while minimizing the labelling costs. Moreover, Rekognition uses an existing trained \nmodel to classify/label the images, making it a more cost -effective solution compared to Ground Truth, which \nuses human labellers. \n2.2. Data Labelling \nFor our data labelling process, we leveraged Amazon Rekognition4, an image and video analysis service provided \nby AWS. We combined images from multiple sources, including the NudeNet classifier dataset, nsfw data scraper  \nNSFW images and 50,000 safe/borderline ima ges. Basic preprocessing (removing corrupted images, invalid \nimage format) was also done prior to uploading onto the S3 Bu cket. We used Amazon Rekognition's \n\"DetectModerationLabels\" function to generate paren t labels and child sub -labels for each NSFW imag e. After \nreviewing the labels, we selected images based on their sub -labels to balance our dataset. We then created \nimage labels and copied the images into different folders within an Amazon S3 bucket based on their new labels. \nWith the number of sub -labels o f each NSFW image, it will be  useful to i nform community m anagers and \noffenders why the images are classified NSFW (\u201cBorderline Nudity\u201d) so as opposed to a Safe/NSFW classification. \nDespite the need for greate r training images, t he team feels that this will also allow the model to be more \nresilient against future content drifts. \n2.3. Dataset Statistics \nFigure D in the Appendix describes the number of labelled datapoints that the team has collected for training.  \n2.4. Data Imbalance \nBased on our sampled data, we have identified that there is a tendency for class imbalance. We will address this \nin our data preprocessing step.  \n2.5. Dataset Format \nOur dataset will be stored in an AWS S3 bucket with a labelled folder structure for easy data access. We will use \nAmazon SageMaker to run the entire machine l earning workflow, including data pre -processing, feature \nengineering, model training, tuning, evaluation, deployment, and monitoring.  \nAmazon SageMaker tools will facilitate data reading, processing, and transformation. Feature engineering will \nextract meaningful image features for improved model perfor mance. We will explore using  transfer learning \nmodels such as ResNet50 and ViT models. The trained models will be deployed to an endpoint for prediction on \nnew data. These pipeline architectures ensure effective and efficient explicit nudity detection using machine \nlearning. \n2.6. Data Privacy & Ethical Considerations \nAs our project focuses on explicit nudity detection, we recognize the importance of ensuring the privacy and \nethical considerations of the data we collect. To protect the privacy of individuals, we will ensure that all images \nused in our dataset are appropriatel y anonymized and stripped of any identifying information. Moreover, we \nwill limit access to the data to only those team members who requir e it for the project, and we will store the \ndata securely in accordance with AWS security best practices. Ethically, w e will ensure that our project is not \nused for any malicious or harmful purposes and that the project's end goal serves a legitimate purpose in society. \nWe will also follow all relevant laws, regulations, and guidelines related to the use of explicit imagery for research \n \n3 See Appendix: Figure B for the Data Collection Pipeline \n4 See Appendix: Figure C for the list of categories classified by Amazon Rekognition", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 4190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37e23a5b-5813-4d7a-9b22-685a34bc8ee3": {"__data__": {"id_": "37e23a5b-5813-4d7a-9b22-685a34bc8ee3", "embedding": null, "metadata": {"page_label": "5", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c58f3433-543d-4a76-89e9-6c5cd909c7ce", "node_type": "4", "metadata": {"page_label": "5", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "887e0e8b3d2f410448951865d71310a85fd71de38b53dbb79969cffb0f135e6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 \npurposes. Finally, we will be transparent about our data collection and labelling process and provide clear \ncommunication to our stakeholders on how the data will be used an d protected. \n3. Machine Learning System Architecture \nThe team has segmented the pipeline into a few components for the purpose of the project5.  \nService Explanation \nAmazon S3 Training Bucket Stores training images that will be converted into PyTorch Tensor \nfor model training \nAmazon S3 Interim Bucket Stores reported/appealed images for moderators to evaluate and \ntake appropriate action. \nAmazon SageMaker \u2013 Training Conduct model training, building and compilation \nAmazon SageMaker \u2013 Model Registry Contains a catalogue of models to track and manage \nAmazon SageMaker - Endpoint Deploying model for real-time inference \nAWS CodeCommit Store source code and changes history  \nAWS CodeBuild Compiles source code and build model  \nAWS CodePipeline Automate pipeline for CI/CD \nAmazon CloudWatch Monitor model performance from logs and to send alarm \nAWS Lambda Serverless computing service to perform inference, update data \nlabel, and trigger model training pipeline. \nAPI Gateway Managed s ervice that facilitates interactions between public \nrequests to AWS services. \n \n3.1. Model Building6 \n3.1.1. Model Building Workflow7 \nIn the development of the model, the team implemented a continuous integration approach. The \ncommencement of this process is signaled when the model building code is committed into the repository. This \nsubmission sets off a CloudWatch event, which in turn initiates the model training pipelin e in CodePipeline. \nThe model training pipeline engages the SageMaker Pipeline to carry out various stages of training. These stages \nare listed as follows: \n1. The preprocessing of training data. \n2. The actual training of the model. \n3. The evaluation of the model. \n4. The final step, which involves registering the model into the Model Registry.  \nDuring the evaluation stage, the trained model must reach a predefined level of accuracy before it is added into \nthe model registry. This requirement is put in place to guarantee t hat any newly trained model satisfies the \nbaseline performance standards for the model. \n3.1.2. Data Preprocessing \nIn the data preprocessing stage, the team will be extracting up to 1000 images per class and adopting 80/10/10 \nsplit of training, validation and test set. This is to ensure that enough data will be used for model training, while \naddressing the class im balance issue and cost consideratio ns. The images will then undergo a series of \ntransformation to ensure that the images conform to the requirements of the model (224x224, normalized).  \nWe have also use various data augmentation methods on the training set, such as random horizontal and vertical \nflips and rotation. These augmentation techniques will help increase the amount of training data , reducing risk \nof overfitting, and improve model generalization by introducing diverse variations in the augmented images.  \n \n5 See Appendix: Figure E for the final system architecture. \n6 See Appendix: Figure F for the detailed model building architecture. \n7 See Appendix: Figure G for the CodePipeline stages for model building.", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 3248, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3130f783-22c5-4215-a5b5-5afd1f4a427e": {"__data__": {"id_": "3130f783-22c5-4215-a5b5-5afd1f4a427e", "embedding": null, "metadata": {"page_label": "6", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e783808c-8ace-4997-bec7-0c5c47ec10c8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "691652a8c6c79f4b21e58822b629180e21eba357bd883ef86eade3f54736a73a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 \n3.1.3. Model Training \nResNet50 \nResNet 50 is a deep convolutional neural network that employs residual networks. It introduced skip connections \nto address the vanishing gradient problems, enabling the training of deeper networks. (Kaiming He, et.al, 2015) \nWhile at its inception it achieved the state-of-the-art results, other model architectures have since surpassed it. \nHowever, it remains as one of the more popular models due to the simplicity of understanding the model.  \nVision Transformer (ViT-16)  \nVision Transformer is an image clas sification architecture that employs transformer architecture to process \nimages. It divides the input images into patches and process them through a series of self -attention layers, \nenabling long-range interaction between image elements. The model also eliminates the need for convolutional \nlayers, allowing us to capture global dependencies in the image.  \nIn the training step, the team has frozen all the feature extraction layers, finetuned the last fully  connected \nclassifier layer of the following models:  \nModel Accuracy \nResnet50 ~20% \nViT-16 ~60% \n \nBased on the model performanc e, we have identified that the ViT -16 will be the most appropriate  as it \noutperforms the rest of the models. While the model can be further fine-tuned to achieve better performance, \nthe team will be utilizing the model after 10 epochs of finetuning due to resource constraints.  \n3.1.4. Model Quantization \nAs the model size can get quite substantial, we have introduced post-training quantization to reduce the  \nprecision of weights , allo wing for compressi on of models while retaining simila r performance.  While the \ncompression of model by way of a reduction in precision results in a degradation of model, the team has built in \na conditional step, where the quantized model will be benchmarke d against the un-quantized model based on \naccuracy. Ultimately, the un-quantized model was deployed as the deviation was greater than 5% threshold set. \nThe un-quantized model size was also relatively manageable at around 300mb.  \n3.2. Model Deployment8  \n3.2.1. Model Deployment Workflow9 \nOur project employs an image classification model designed to operate in a real -time inference setting. Given \nthe time-sensitive nature of our task and the potential impact of erroneous classi fications, we have chosen a \ndeployment workflow that maximizes both model accuracy and system  reliability. \nOur workflow is designed as a sequence of steps: Build, Deploy to Staging, Approve Production Deployment, and \nDeploy to Production. The workflow init iated either when modifications a re made to the model deployment \nsource code or when a new model gets approved in the model registry . The workflow then builds a package \nfrom the repository, which encompasses both our staging and production deployment CloudFormation \ntemplate. \nUtilizing the template, the workflow updates the stacks in CloudFormation. This either results in the creation or \nthe update of the SageMaker endpoint in the staging environment. Following these updates, we execute an \ninference test on the staging endpoint. \nThe `Approve Production Deployment` stage, a manual checkpoint, is the gatekeeper in preventing the workflow \nto automatically deploy the model into production environment. At this stage, the team could conduct \nadditional testing on the staging endpoint. Based on the results of these tests, the team can decide whether to \nauthorize the deployment into the production environment. \n \n8 See Appendix: Figure H for the detailed model deployment architecture. \n9 See Appendix: Figure I for the CodePipeline stages for model deployment.", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 3680, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07c964c0-42f7-488c-aea9-20a4cc7221bd": {"__data__": {"id_": "07c964c0-42f7-488c-aea9-20a4cc7221bd", "embedding": null, "metadata": {"page_label": "7", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3611965f-3d94-46d6-991c-3f9797533747", "node_type": "4", "metadata": {"page_label": "7", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "9f11adb6e347f7142e9291392d1378464875da500140366aab737cdd16935547", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 \nOnce the stage approval is given, which happens after successful testing in the staging environment, the \nworkflow proceeds to deploy the model.  The deployment strategy used is determined by the specifications \nwithin the CloudFormation template included in the package.  \n3.2.2. Auto Scaling Policy \nGiven the nature of the business use case, being able to automatically scale the endpoint instance horizontally \nwill be essential to ensure steady performance with the appropriate cost trade-offs. With this, we made use of \nthe \u201cSageMakerVariantInvocationsPerInstance\u201d metric, monitoring a target value of 70 per minute, with a scale \nout cooldown of 5 mins an d scale in cooldown of 10 mins. These are ten tative values and will be adjust ed \naccordingly to fit individual communities as they see fit.  \n3.2.3. Deployment Strategy \nThe team adopted Canary deployment strategy in the deployment workflow. Canary deployment  is a type of \nincremental rollout process where new versions of a model (or application) are released to a small, controlled \nsubset of users or environment before a full rollout. This deployment strategy allows us to test the model's \nperformance, assess potential risks, and detect issues early without affecting the entire user base or system.   \nIn the context of our project, using the Canary Deployment strategy for our explicit image classification model \noffers several benefits. Firstly, given that our system operates in a real-time environment, it's critical to ensure \na seamless experience for end -users. By initially deploying the new model version to a limited subset of traffic, \nwe can monitor its performance, measure prediction accuracy , and identify any u nexpected behaviours or \nanomalies before it affects all users. \nSecondly, this approach provides us an opportunity to compare the new model version with the existing one in \na live setting. We can evaluate metrics such as model latency, throughput, and resource usage under actual load \nconditions. Such direct comparison under real -world conditions provides valuable feedback to inform our \ndecision about the full deployment of the new model. \nFinally, the incremental rollout reduces the risk a ssociated with deploying new models. If any problems arise \nduring the Canary phase, we can quickly rollback the deployment, minimizing the impact on the overall system \nand user experience. It also gives us time to diagnose the issue and make necessary adju stments before  a \nbroader rollout. Thus, Canary Deployments act as an essential safety net, ensuring high reliability and \nperformance consistency of our image classification system. \nThe team has added CloudWatch alarms that are  used for managing rollback pr ocedures durin g Cana ry \ndeployments. For th is purpose, the chosen metric is `InvocationModelErrors`.  This alarm is available in both \nstaging and production environment. \n3.3. Monitoring & Retraining Step \n3.3.1. User Feedback to handle Concept & Model Drift \nConcept drift arises when the underlying data distribution & statistical properties evolve, rendering the model's \nassumptions invalid. It can be triggered by factors such as shifting user preferences, market dynamics, or external \ninfluences. Detecting and adapting to concept drift is essential for maintaining accurate predictions in dynamic \nenvironments, making the model suitable for communities with different social acceptance norms. \nOn the other hand, model drift refers to the degradation of model performance over time, even without changes \nin the data distribution. It can be caused by shifts in the operating environment, emerging patterns, or limitations \nof the model itself. Given our numerous labels, our model will be more suscepti ble to such shifts through \niterations of re training on predicted images . Monitoring and addressing model drift are crucial to uphold the \nreliability and effectiveness of the machine learning model.", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 3947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "38ebd667-7939-4ac9-8a8b-42906bb73769": {"__data__": {"id_": "38ebd667-7939-4ac9-8a8b-42906bb73769", "embedding": null, "metadata": {"page_label": "8", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc9fabfe-5bca-4c51-acd4-e4f7c1a404a9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "78f76bca3ccb2fbad172070d0943b01bd2a60fb9d0d13d0ca8e7c6fb70907e6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 \n3.3.2. Implementation of User Feedback \u2013 Discord Server Bot \nFor our project, we decided to implement this user feedback loop in Discord. A popular discord bot template10 \nwas modified to allow the bot to do the following: \nContext Bot Actions \nUser uploads image  Send the url containing the image to the  ModelUpload Lambda through a POST API, \nwhich simultaneously sends the image to Sagemaker endpoint, staging S3 bucket and \nAWS RDS (to store the metadata), returning the RDS file id and the classification result. \nThese are saved in a local database within the bot along with other metadata (i.e. user, \nchannel, timestamp) \nImage result \u2013 Safe Does nothing \nImage result - NSFW Auto-Moderating actions: Timeout the user (10 seconds) a nd d eletes the message . \nSends a message in the moderator notification channel with relevant details. Sends a \nprivate message to the user with reason for the timeout and gives an option to appeal. \nNSFW user \u2013 Appeal Retrieves the RDS file id and send it to the ModelAppeal Lambda through a POST A PI, \nupdating the appeal status in RDS and returning the appeal id. Sends a message in the \nmoderator notification channel with relevant details. \nNSFW user \u2013 Accept Does nothing \nUser reports image Sends a message in the moderator notification channel with relevant details. \n \nBy incorporating user feedback , involvement in model validation plays a vit al role in detecting and miti gating \ndrift. Users' interactions with the model's predictions through community engagement provide valuable insights \ninto its performance. Whenever images are wrongly classified (via the appeal/report loop), moderators will then \ncheck/confirm the labels of th ose images, moving them  into the training bucket to form the ground t ruth. \nCurrently, images that are not reported will also be moved by the moderators/administrator every 24 hours to \nthe training bucket.  \nWhenever the numb er of wrongly  classified images crosses a pre -defined threshold, the lambda function will \ntrigger the model training pipeline. \nThis implementation can be appli ed to any other online community in a similar fashion. Given the modularized \nnature of our project, the code can be used to build a separate pipeline in another AWS account. Community \nmanagers can then update the various API parameters for their own user feedback mechanism implementation.  \n3.3.3. AWS CloudWatch Alarms \nAs part of our real -time explicit image classification project, we've identified the necessity to closely monitor \ncertain metrics in our machine learning (M L) system to ensure optimal perf ormance and efficiency.  These \nmetrics, accessible through Amazon CloudWatch11, provide valuable insights into our model's performance and \ncan trigger necessary adjustments in the infrastructure or the model itself if predefined thresholds are breached. \nThe team added a few alarms on SageMaker endpoint in both the staging and production environments, aiming \nto facilitate the monitoring process. \nWe also set an \u201cInvocation Model Errors\u201d CloudWatch alarm to monitor the number of failed invocations of our \nimage classification model in the production environment. This alarm is pivotal as it provides immediate \nnotification of spikes in error counts,  which could signify serious issues with th e model or the input data. This \nenables us to rapidly investigate and rectify any issues, maintaining a reliable service and ensuring a superior \nuser experience, which is crucial for a real-time inference pipeline like ours. We set the alarm threshold to more \nthan 5 model invocation errors in 5 minutes.  Furthermore, this alarm is integrated into our deployment \nconfiguration. If the alarm is triggered during deployment, SageMaker  will initiate an automatic rollback  of the \ndeployment process.  By including this alarm in our deployment workflow, we strengthen the robustness and \nreliability of our machine learning system. \n \n10 Discord Bot Template from https://github.com/kkrypt0nn/Python-Discord-Bot-Template \n11 See Appendix: Figure J for the list of CloudWatch alarms.", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 4095, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb9e84fe-3085-490d-9784-7a511c57a769": {"__data__": {"id_": "fb9e84fe-3085-490d-9784-7a511c57a769", "embedding": null, "metadata": {"page_label": "9", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b15423d3-f473-4e7a-82ff-27423f1df479", "node_type": "4", "metadata": {"page_label": "9", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "fcd3fa019549018ec14117d2dd150977ce2f70078d979da7688d088da011264a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 \nAnother essential metric we chose to monitor is CPU Utilizatio n. While we also have autoscaling po licy in each \nof the endpoint, keeping a watchful eye on the CPU usage of our model host can offer valuable insight regarding \nmodel computational demands.  Such information might necessitate optimization of the model to re duce its \ncomputational load or an up grade to a larger instance type to handle the model's demands better. Despite the \nautoscaling policy we have , closely monitoring CPU usage provides us with early warnings regarding potential \ndisruptions caused by  insufficient computing power.  We set the al arm threshold to more than an average of \n70% CPU usage (on average) for every 5 minutes interval based on baseline percentages.  \nIn essence, integrating CloudWatch monitoring into our ML system allows us to respond promp tly to \nperformance issues, streamline computational resources, and, ultimately, ensure the provision of a high-quality, \nreal-time explicit image classification service. \n4. Limitations, Considerations & Future Works \n4.1. Technical Limitations  \nObtaining Feature Attribution with SageMaker Clarify Model Explainability12 \nAn essential future enhancement for our image classification pipeline is the real -time monitoring of model \nexplainability, applied both during training an d on live data. This augmentation would  substantially increase \nsystem transparency and robustness by providing instant insights into the model's decision-making process. This \ninvolves not only tracking performance metrics but also analysing the feature attributions given by SageMaker \nClarify's SHAP values in real-time. \nSHAP (SHapley Additive exPlanations) values offer a powerful tool in understanding feature importance. \nOriginating from cooperative game theory, SHAP assigns each input feature an importance score, offering a \nbreakdown of how each influences the model's prediction. For our image classification model, SageMaker Clarify \ncan indicate the image regions most influential in making specific predictions, assisting us in discerning whether \nthe model focuses on relevant parts or is distracte d by background noise. This clarity is represented through a \n\"heatmap\" which highlights the most influential areas of the image, thereby providing greater transparency and \naccountability to the model's decision-making process. \n4.2. Data Limitations \nAmazon Rekognition was chosen as the data labelling solution as opposed to A mazon GroundTruth primarily \ndue to cost con cerns. This subjects the model to  the inherent biase s/distributions from Amazon Rekognition. \nNevertheless, these should be mitigated in the lon g run through iterations of model retraining with the model \nadapting to the individual communities\u2019 user feedback. \n4.3. Model Limitations \nModel Accuracy & Experimentation with Proportion of Class Labels  \nThe current model accuracy stands at around 60%. While this shows some capacity for explicit content detection, \nthere is considerable room for improvement. One potential avenue for enhancing the model's precision involves \nexpanding the dataset utilized during training. Having a more diverse, extensive dataset would allow the model \nto learn and distinguish nuances in images more effectively. Increasing the number of training epochs could also \nyield benefits; it allows the model additional opportunities to le arn from the data. However, it's crucial to \nbalance this with computational resources and the risk of overfitting.  \nBalancing the proportion of safe to Not Safe for Work (NSFW) images in model training is crucial for our explicit \ncontent moderation project. The costs of false negatives, where NSFW images are incorrectly deemed safe, are \nconsiderably higher than false positives due to potential user exposure to explicit content and subsequent harm \nto the platform's reputation. Therefore, we may opt for a model more inclined to predict NSFW, even if it slightly \nincreases false positives. However, we must be cautious to avoid overfitting the model to t he training data. To \n \n12 See Appendix: Figure K for an example of explainability on image classification with SageMaker Clarify .", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 4192, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f9a61d15-4f33-45b3-b0f6-536e7b3ca5ba": {"__data__": {"id_": "f9a61d15-4f33-45b3-b0f6-536e7b3ca5ba", "embedding": null, "metadata": {"page_label": "10", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eecb45a3-4c28-4a4d-a132-96c183ea2b89", "node_type": "4", "metadata": {"page_label": "10", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "e685a4cdecfb6a44f8aaae9a2e1badab227375d0a82f444227786c9b5834b004", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 \nachieve this balance, we will rigorously experiment with different data proportions to fin d an optimal \nperformance level in real-world conditions. \n4.4. Deployment Infrastructure \nModel Latency \nAnother key metric to focus on is Model Prediction Latency. This measure refers to the duration it takes for our \nmodel to generate a prediction upon receiving an input. Latency plays a significant role in t he user experience, \nparticularly for real -time applications like ours. For this reason, we plan to monitor the  ModelLatency metric \nunder the AWS/SageMaker namespace in CloudWatch. By defining an acceptable threshold for latency based \non our application's r equirements, we can set up CloudWatch alarms to notify us if this limit is exceeded. This \napproach allows us to maintain the responsiveness of our service and ensure a seamless user experience. \nSetting up a suitable baseline for Model Prediction Latency is essential to adequately monitor and react to \npotential issues in real -time. As we move towards a stagin g test with general users, we will begin collecting \nlatency data under real-world conditions. This data will help us understand the typical latency our model exhibits \nunder varying load and user interaction patterns. \nIn this staging phase, we will observe and analyze the trends and pat terns of model latency. We will consider \nboth average latency and peak times, accounting for user behavior patterns that  might impact system load. By \nobserving these patterns, we will be able to set a realistic and acceptable threshold for ModelLatency. Our aim \nis to set a baseline that accounts for typical usage, while also ensuring we can react swiftly if latency starts t o \nexceed expected peaks, ensuring our system continues to deliver timely responses and a seamless user \nexperience. \nAdversarial Attacks \nThe model may be susceptible to adversarial attacks, where users intentionally provide inaccurate feedback or \nsubmit images designed to mislead the model. These attacks can degrade the model's performance over time, \nleading to an increase in misc lassifications. Implementing robust verification processes for user feedback and \ndeploying \u201cdefences\u201d against adversarial attacks can help to mitigate this risk. \nPipeline Architecture \nOur current implementation makes use of a real -time inference. Switching to an asynchronous inference setu p \nmay be more justifiable as the use case scales up.  \n4.5. Ethical & Legal Considerations \nUsing user images for model training raises significant ethical concerns, primarily revolving around privacy and \nconsent. While the images could significantly improve model performance due to their real -world variability, \nusers might oppose their personal content  being used for such purposes, even if the images are anonymized. \nAdditionally, considerations around the handling of potentially explicit images, especially those involving minors \nor non-consenting individuals, add layers of complexity. Addressing these c oncerns necessitates stringent data \nhandling and usage policies, with user consent at the forefront. \n4.6. Scope Expansion \nWhile the current project focuses on detecting explicit nudity, the reality of content moderation extends to other \npotentially harmful or inappropriate material such as gore, violence, drug -related content, as w ell as different \nmedia formats like GIFs and videos. Expanding the project scope to handle these elements would increase the \nsystem's overall effectiveness but also introduce additional complexities. Each type of content and media format \nmight require different detection techniques and algorithms, which would need to be seamlessly integrated into \nthe existing infrastructure.", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 3729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "548a4ff0-bc4b-4138-9f67-0de7c8c8fd61": {"__data__": {"id_": "548a4ff0-bc4b-4138-9f67-0de7c8c8fd61", "embedding": null, "metadata": {"page_label": "11", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ced3bcf8-e511-427b-842b-6c6e3951987b", "node_type": "4", "metadata": {"page_label": "11", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "9f14d115d7b9862f5124500409bb382ee96d2d43cfb1035e9f12df95f11096bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 \n5. References \n \nAlex000kim, Nsfw_Data_Scraper, (2022). GitHub repository, \n https://github.com/alex000kim/nsfw_data_scraper \nAmazon Web Services (2020). Explaining Image Classification with SageMaker Clarify. Amazon SageMaker \nExamples. https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-\nclarify/computer_vision/image_classification/explainability_image_classification.html \nBrown, R. (2023, May 9). Why social media content moderation is important for online plat forms & how it \nworks?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-important-\nfor-online-platforms-how-it-works/ \nCogito Tech LLC. (2023, May 9). Why social media content moderation is important for online platforms &amp; \nhow it works?. Cogito. https://www.cogitotech.com/blog/why-social-media-content-moderation-is-\nimportant-for-online-platforms-how-it-works/ \nEBazarov, Nsfw_Data_Source_Urls, (2022). GitHub repository, \n https://github.com/EBazarov/nsfw_data_source_urls \nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  (2015, December 10). Deep Residual Learning for Image \nRecognition. arXiv:1512.03385. Retrieved from https://arxiv.org/abs/1512.03385 \nKkrypton, Python Discord Bot Template (2023). GitHub repository,                  \nhttps://github.com/kkrypt0nn/Python-Discord-Bot-Template \nModerating content (2023). Amazon Rekognition Developer Guide. Retrieved from  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/moderation.html \nMatheus Oliveira Franca (2021, June 29). Detection and categorization of suggestive thumbnails. Retrieved \nfrom https://www.diva-portal.org/smash/get/diva2:1595278/FULLTEXT01.pdf \nNotAI.tech, Nudenet, (2022). GitHub repository, https://github.com/notAI-tech/NudeNet \nT.J. Thomson, Daniel Angus, Paula Dootson. (2022, December 21). 3.2 billion images and 720,000 hours of \nvideo are shared online daily. can you sort real from fake?. The Conversation. \nhttps://theconversation.com/3-2-billion-images-and-720-000-hours-of-video-are-shared-online-daily-\ncan-you-sort-real-from-fake-148630", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 2055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8fb81d93-cca7-420e-9756-5c237cf0ce4b": {"__data__": {"id_": "8fb81d93-cca7-420e-9756-5c237cf0ce4b", "embedding": null, "metadata": {"page_label": "12", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d96344b-702e-44ae-a628-c3def64cb9d4", "node_type": "4", "metadata": {"page_label": "12", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "531455696fd78a41ee3508e516d800a787cbf57f6851a6e13f39b77761f106a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11 \n6. Appendix \n \nFigure A: Types of Contents Moderated on Social Media Platforms (from Cognito) \n \n \nFigure B: Data Collection Pipeline\n \n \nFigure C: Amazon Rekognition Categories (from Amazon Developer Guide)", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "30b20683-4038-4104-b233-ec5b5e278f68": {"__data__": {"id_": "30b20683-4038-4104-b233-ec5b5e278f68", "embedding": null, "metadata": {"page_label": "13", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "292ae60f-2ad5-45f6-a48a-dc1d6723ecf0", "node_type": "4", "metadata": {"page_label": "13", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "d78ff4143109b715a6dd7fc06bdb1843d085d1aac8a36a385c32f4e1d104211e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 \nFigure D: Dataset Statistics\n \n \nFigure E: Final Overall System Architecture", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 81, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b693e1bb-78e2-4fd6-ada4-1cae0a81366d": {"__data__": {"id_": "b693e1bb-78e2-4fd6-ada4-1cae0a81366d", "embedding": null, "metadata": {"page_label": "14", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4edeab6d-1648-4e23-bde3-5cd9d208a703", "node_type": "4", "metadata": {"page_label": "14", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "19b9769b99e29f8c7879bdedd702652334fedc42c415b42a179e2ca772f35ffc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13 \nFigure F: Detailed Architecture for Model Building\n \n \nFigure G: CodePipeline Stages for Model Building", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae07d45d-345f-43fe-9c29-d0c21af04a44": {"__data__": {"id_": "ae07d45d-345f-43fe-9c29-d0c21af04a44", "embedding": null, "metadata": {"page_label": "15", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d14df1ff-70a0-4665-98b9-13fbef3395a4", "node_type": "4", "metadata": {"page_label": "15", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "9d6c3cbc44077e0a884897188694e9dcda7b78345532172cf1b8476b029e6ed8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 \nFigure H: Detailed Architecture for Model Deployment", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 57, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e8b9906b-8855-436e-bb9d-1c7d00641d2d": {"__data__": {"id_": "e8b9906b-8855-436e-bb9d-1c7d00641d2d", "embedding": null, "metadata": {"page_label": "16", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1de5b08-9c96-4658-86ca-c5324d408399", "node_type": "4", "metadata": {"page_label": "16", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "a744cd5d0b86d781fb64b2d89cb120f0089f467356bc1823dec4692311df5bf3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15 \nFigure I: CodePipeline Stages for Model Deployment", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 55, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4b826ae5-0c1d-4553-afef-104ff899f25a": {"__data__": {"id_": "4b826ae5-0c1d-4553-afef-104ff899f25a", "embedding": null, "metadata": {"page_label": "17", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bea8a6f9-1886-4aae-9e6f-89175ae327de", "node_type": "4", "metadata": {"page_label": "17", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "b112ab41dc42e0ef798a50b6b41134f35596d78cf0cdf81dc8bc9eb7170b81f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16 \nFigure J: Cloudwatch Alarms \n \nFigure K: SageMaker Clarify Example (from Amazon SageMaker Examples)", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d04865cb-39b7-4ddc-8075-b9968c66137d": {"__data__": {"id_": "d04865cb-39b7-4ddc-8075-b9968c66137d", "embedding": null, "metadata": {"page_label": "1", "file_name": "training-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/training-report.pdf", "file_type": "application/pdf", "file_size": 255497, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb63d3cf-e16b-4268-8f91-f66dfea2927b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "training-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/training-report.pdf", "file_type": "application/pdf", "file_size": 255497, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "e338adc54fb7a989945b2f31c87551ec7d1be4cf9f2ee106a2f9702b55cb988f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Training Report \u2013 Results, Evaluation and Future works [Part I] Comparison of Model Performance (Pre-trained base model vs Fine-tuned models) For our model evaluation on Automatic Speech Recognition (ASR) tasks, the selected evaluation metric is Word Error Rate (WER). Even though Character Error Rate (CER) penalizes minor spelling errors much less as compared to WER, WER evaluates the system's ability to learn more about the context of predictions (in English language). Thus, is more widely used to access the performance of speech systems.  We evaluated the fine-tuned \u201cwav2vec2-large-960h\u201d ASR model\u2019s inference capabilities against the performance of the pre-trained \u201cwav2vec2-large-960h\u201d baseline model development set (cv-valid-dev). Key dataset features and results are displayed in Table 1. Model name Type Dataset Dataset size WER score \u201cwav2vec2-large-960h\u201d Pre-trained base model  cv-valid-dev 4,076 10.8% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-dev 4,076 7.7% \u201cwav2vec2-large-960h\u201d Fine-tuned (2,000 files) cv-valid-test 3,995 12.0% \u201cwav2vec2-large-960h\u201d Fine-tuned (6,300 files) cv-valid-test 3,995 7.3% Table 1: Comparison on pre-trained base model vs fine-tuned model on development set WER from using pre-trained \u201cwav2vec2-large-960h\u201d model (without fine-tuning) was approximately 10.8% while WER using fine-tuned \u201cwav2vec2-large-960h\u201d model was 3-percentage points lower at 7.7%. A better performance attributed to model fine-tuning can be attributed to better alignment to domain-specific data (common voice datasets), i.e. being able to capture the dataset\u2019s unique nuances like accent, gender, age and noise distribution. A key feature to speech variability is identified to be \u201caccent\u201d. We found that \u201caccent\u201d distributions across training and test sets were consistent, possibly explaining an improved fine-tuned performance. Following model inference on the development set, we observed the distribution of WER metrics across our key feature \u201caccent\u201d and compare our two models (refer to Figure 1 below).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2047, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de812c97-885e-4611-959b-b8a85acd75ea": {"__data__": {"id_": "de812c97-885e-4611-959b-b8a85acd75ea", "embedding": null, "metadata": {"page_label": "2", "file_name": "training-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/training-report.pdf", "file_type": "application/pdf", "file_size": 255497, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "16714dde-21c4-4c0b-9932-e3adf500b4fb", "node_type": "4", "metadata": {"page_label": "2", "file_name": "training-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/training-report.pdf", "file_type": "application/pdf", "file_size": 255497, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "17884dbe2acc78cbbe847b3cbc74860ecb76a8c101dc14a0872aebcbb4bc8dac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1: WER scores by \u201caccent\u201d \u2013 Baseline test (pre-trained wav2vec2-large-960h) vs Fine-tuned model (6,300 records) From Figure 1, we observed that the baseline model performs well on speech/audio data from regions like the US, Canada and England. The key question now is: How does fine-tuning affect performance across regions? Our fine-tuned model shows improvements in WER scores across most other regions, indicating successful accent mapping. Notably, countries like Singapore and Africa recorded strong improvements while countries like Philippines and India shows less improvements. This could be due to unique speech nuances and pronunciations and more work needs to be done to explore potential solutions. [Part II] Propose series of steps, including datasets and experiments to improve accuracy of fine-tuned wav2vec2 model 1. Dataset Diversification and Augmentations Papers have shown that audio augmentation strategies has led to minor improvements in evaluation scores. In particular (Ko et.al., 2015) demonstrated the benefits of speech perturbations on model performance. Hence, exploring other strategies like speech perturbations, time masking, pitch shift and background noise injection might be beneficial in contributing to a more diverse training dataset, which could be crucial in improvements in model\u2019s generalisability to unique accents like those in India or the Philipines. 2. Integrating External Language Models for enhanced performance. Leveraging Large Language Models (LLMs) for speech recognition is another feasible solution to improve fine-tuning evaluation accuracy. This post-processing strategy (after acoustic model decoding) involves integrating a transformer-based LLM decoder to perform speech recognition as next token prediction (Hono et.al., 2023). In the context of HuggingFace processors, we can implement one with a decoder that includes an Language Model such as \u201cFacebook/wav2vec2-large-960h-lv60-self\u201d. As it was observed that there were some spelling mistakes contributing to error percentages in WER, these context-aware corrections and output re-ranking strategy could potentially improve WER accuracy in speech transcriptions after model fine-tuning. 3. Hyperparameter Tuning and Fine-tune Model over entire \u201ccv-valid-train\u201d Dataset (195,776 records) Our current approach used open-source past projects as reference points for hyperparameter settings. Some sources include HuggingFace articles (with example colab notebooks), Medium and \u201creadthedocs\u201d articles. In future experiments, we could incorporate methodologies such as random search or Bayesian optimisation to determine optimal hyperparameters for fine-tuning our wav2vec2 model. Another key limitation of this project is compute and memory limitations. We were only able to fine-tune our pre-trained  \u201cwav2vec2-large-960h\u201d model on 6,300 audio files. Therefore, if resources permit, utilizing a large dataset for fine-tuning, coupled with hyperparameter tuning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2979, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3885cd0a-0f92-4f48-916b-331c55cb9f83": {"__data__": {"id_": "3885cd0a-0f92-4f48-916b-331c55cb9f83", "embedding": null, "metadata": {"page_label": "3", "file_name": "training-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/training-report.pdf", "file_type": "application/pdf", "file_size": 255497, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1e3138d-d435-4dc6-bfce-1af32afb4d57", "node_type": "4", "metadata": {"page_label": "3", "file_name": "training-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/training-report.pdf", "file_type": "application/pdf", "file_size": 255497, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}, "hash": "0d05648ebfa495e11cbb458d23b88530e21c59efbc64171eca47ec70e08ba17d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "to optimize model training might improve overall evaluation performance of the pre-trained model, leading to more accurate inferencing results. 4. Exploration of Other Methodologies to Enhance Training Data Quality for Model Fine-tuning Conventional strategies like dataset augmentation and the integration of external language models have been shown to improve model fine-tuning performance in WER scores. Inspired by Guo et. al., 2024, we recommend experimenting with a semi-supervised learning strategy where we utilise self-transcribed, high confidence data to supplement the training data pool for model fine-tuning. These transcribed data can be selected based on model confidence levels (eg. WER <= 0.3).  [Part III] Conclusion The fine-tuning of the wav2vec2-large-960h model on the Common V oice dataset resulted in a notable WER improvement over the baseline model, demonstrating the benefits of domain adaptation. Specifically, fine-tuning allowed the model to better align with accent variations and speech patterns, leading to improved transcription accuracy across diverse regions. However, performance discrepancies across certain accents indicate areas for further refinement. To further enhance inferencing accuracy, we propose a multi-faceted approach involving dataset diversification, augmentation techniques, integration of external language models, and hyperparameter tuning. Additionally, semi-supervised learning strategies could leverage high-confidence transcriptions to expand training data, reducing WER even further. By implementing these enhancements, we aim to develop a more robust and generalizable ASR model, capable of accurately transcribing speech across diverse linguistic and acoustic conditions. References Ko, T., Peddinti, V ., Povey, D., & Khudanpur, S. (2015). Audio augmentation for speech recognition. Interspeech 2015. Retrieved from https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf  Hono, S., Kanda, N., Yoshioka, T., Wu, C., Li, X., & Xiao, X. (2023). Transformer-based language models for speech recognition post-processing. arXiv preprint arXiv:2312.03668. Retrieved from https://arxiv.org/pdf/2312.03668  Guo, J., Liu, Z., Zhang, T., & Chen, C. L. P. (2024). Incremental self-training for semi-supervised learning. arXiv preprint arXiv:2404.12398. Retrieved from https://arxiv.org/abs/2404.12398", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"fc671f47-45ea-43d2-8eaf-c8e6c19a0666": {"node_ids": ["da39b81f-f134-49e1-8cc4-cd8d4b7f0846"], "metadata": {"page_label": "1", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "9166a5ad-9f8c-4239-961c-9b1213e03535": {"node_ids": ["8ac28684-2a61-489b-ae1b-c31b0465e84f"], "metadata": {"page_label": "2", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "a4c6fa81-4dce-48b9-8c79-868b4ce4aef1": {"node_ids": ["4f77933c-ea9b-40d5-a673-0d8a5798b7a1"], "metadata": {"page_label": "3", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "d9adf0d0-964e-4de9-8b2f-ef118bf98890": {"node_ids": ["9fca5882-e3b1-4199-99a5-1abf784622b9"], "metadata": {"page_label": "4", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "c58f3433-543d-4a76-89e9-6c5cd909c7ce": {"node_ids": ["37e23a5b-5813-4d7a-9b22-685a34bc8ee3"], "metadata": {"page_label": "5", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "e783808c-8ace-4997-bec7-0c5c47ec10c8": {"node_ids": ["3130f783-22c5-4215-a5b5-5afd1f4a427e"], "metadata": {"page_label": "6", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "3611965f-3d94-46d6-991c-3f9797533747": {"node_ids": ["07c964c0-42f7-488c-aea9-20a4cc7221bd"], "metadata": {"page_label": "7", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "bc9fabfe-5bca-4c51-acd4-e4f7c1a404a9": {"node_ids": ["38ebd667-7939-4ac9-8a8b-42906bb73769"], "metadata": {"page_label": "8", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "b15423d3-f473-4e7a-82ff-27423f1df479": {"node_ids": ["fb9e84fe-3085-490d-9784-7a511c57a769"], "metadata": {"page_label": "9", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "eecb45a3-4c28-4a4d-a132-96c183ea2b89": {"node_ids": ["f9a61d15-4f33-45b3-b0f6-536e7b3ca5ba"], "metadata": {"page_label": "10", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "ced3bcf8-e511-427b-842b-6c6e3951987b": {"node_ids": ["548a4ff0-bc4b-4138-9f67-0de7c8c8fd61"], "metadata": {"page_label": "11", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "1d96344b-702e-44ae-a628-c3def64cb9d4": {"node_ids": ["8fb81d93-cca7-420e-9756-5c237cf0ce4b"], "metadata": {"page_label": "12", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "292ae60f-2ad5-45f6-a48a-dc1d6723ecf0": {"node_ids": ["30b20683-4038-4104-b233-ec5b5e278f68"], "metadata": {"page_label": "13", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "4edeab6d-1648-4e23-bde3-5cd9d208a703": {"node_ids": ["b693e1bb-78e2-4fd6-ada4-1cae0a81366d"], "metadata": {"page_label": "14", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "d14df1ff-70a0-4665-98b9-13fbef3395a4": {"node_ids": ["ae07d45d-345f-43fe-9c29-d0c21af04a44"], "metadata": {"page_label": "15", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "e1de5b08-9c96-4658-86ca-c5324d408399": {"node_ids": ["e8b9906b-8855-436e-bb9d-1c7d00641d2d"], "metadata": {"page_label": "16", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "bea8a6f9-1886-4aae-9e6f-89175ae327de": {"node_ids": ["4b826ae5-0c1d-4553-afef-104ff899f25a"], "metadata": {"page_label": "17", "file_name": "eyecatcher-project-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/eyecatcher-project-report.pdf", "file_type": "application/pdf", "file_size": 1250235, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "bb63d3cf-e16b-4268-8f91-f66dfea2927b": {"node_ids": ["d04865cb-39b7-4ddc-8075-b9968c66137d"], "metadata": {"page_label": "1", "file_name": "training-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/training-report.pdf", "file_type": "application/pdf", "file_size": 255497, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "16714dde-21c4-4c0b-9932-e3adf500b4fb": {"node_ids": ["de812c97-885e-4611-959b-b8a85acd75ea"], "metadata": {"page_label": "2", "file_name": "training-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/training-report.pdf", "file_type": "application/pdf", "file_size": 255497, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}, "c1e3138d-d435-4dc6-bfce-1af32afb4d57": {"node_ids": ["3885cd0a-0f92-4f48-916b-331c55cb9f83"], "metadata": {"page_label": "3", "file_name": "training-report.pdf", "file_path": "/Users/jinkettyee/Desktop/my_GitHub/great-things/RAG-webscraper/docs/training-report.pdf", "file_type": "application/pdf", "file_size": 255497, "creation_date": "2025-06-02", "last_modified_date": "2025-06-02"}}}}